<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title># Supported models</title>
    <link rel="stylesheet" href="/../../style.css">
</head>
<body>
    <pre><code><hr />
<p>stage: AI-powered
group: Custom Models
info: To determine the technical writer assigned to the Stage/Group associated with this page, see https://handbook.gitlab.com/handbook/product/ux/technical-writing/#assignments
description: Supported models and hardware requirements.
title: Models and hardware requirements</p>
<hr />
<p>{{&lt; details &gt;}}</p>
<ul>
<li>Tier: Premium, Ultimate</li>
<li>Add-on: GitLab Duo Enterprise</li>
<li>Offering: GitLab Self-Managed</li>
</ul>
<p>{{&lt; /details &gt;}}</p>
<p>{{&lt; history &gt;}}</p>
<ul>
<li><a href="https://gitlab.com/groups/gitlab-org/-/epics/12972">Introduced</a> in GitLab 17.1 <a href="../feature_flags/_index.html">with a flag</a> named <code>ai_custom_model</code>. Disabled by default.</li>
<li><a href="https://gitlab.com/groups/gitlab-org/-/epics/15176">Enabled on GitLab Self-Managed</a> in GitLab 17.6.</li>
<li>Changed to require GitLab Duo add-on in GitLab 17.6 and later.</li>
<li>Feature flag <code>ai_custom_model</code> removed in GitLab 17.8.</li>
<li>Generally available in GitLab 17.9.</li>
<li>Changed to include Premium in GitLab 18.0.</li>
</ul>
<p>{{&lt; /history &gt;}}</p>
<p>GitLab Duo Self-Hosted supports integration with industry-leading models from Mistral, Meta, Anthropic, and OpenAI through your preferred serving platform.</p>
<p>You can use:</p>
<ul>
<li>Supported models to match your specific performance needs and use cases.</li>
<li>In GitLab 18.3 and later, your own compatible model to experiment with models beyond the officially supported options.</li>
<li>GitLab AI vendor models to connect to AI models without the need to host your own infrastructure. These models are managed entirely by GitLab.</li>
</ul>
<h2>Supported models</h2>
<p>The AI Gateway supports multiple providers through LiteLLM.</p>
<p>GitLab-supported models offer different levels of functionality for GitLab Duo features,
depending on the specific model and feature combination. </p>
<ul>
<li>Full functionality: The model can likely handle the feature without any loss of quality.</li>
<li>Partial functionality: The model supports the feature, but there might be compromises or limitations.</li>
<li>Limited functionality: The model is unsuitable for the feature, likely resulting in significant quality loss or performance issues.
  Models that have limited functionality for a feature will not receive GitLab support for that specific feature.</li>
</ul>
<!-- vale gitlab_base.Spelling = NO -->

<table>
<thead>
<tr>
<th>Model family</th>
<th>Model</th>
<th>Supported platforms</th>
<th>Code completion</th>
<th>Code generation</th>
<th>GitLab Duo Chat</th>
<th>GitLab Duo Agent Platform</th>
</tr>
</thead>
<tbody>
<tr>
<td>General</td>
<td><a href="https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash">Gemini 2.5 Flash</a></td>
<td><a href="https://cloud.google.com/vertex-ai">Vertex</a></td>
<td>{{&lt; icon name="dash-circle" &gt;}} Limited functionality</td>
<td>{{&lt; icon name="dash-circle" &gt;}} Limited functionality</td>
<td>{{&lt; icon name="dash-circle" &gt;}} Limited functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
</tr>
<tr>
<td>Mistral Codestral</td>
<td><a href="https://huggingface.co/mistralai/Codestral-22B-v0.1">Codestral 22B v0.1</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>Mistral</td>
<td><a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506">Mistral Small 24B Instruct 2506</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>Claude 3</td>
<td><a href="https://www.anthropic.com/news/claude-3-5-sonnet">Claude 3.5 Sonnet</a></td>
<td><a href="https://aws.amazon.com/bedrock/claude/">AWS Bedrock</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
</tr>
<tr>
<td>Claude 3</td>
<td><a href="https://www.anthropic.com/news/claude-3-7-sonnet">Claude 3.7 Sonnet</a></td>
<td><a href="https://aws.amazon.com/bedrock/claude/">AWS Bedrock</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
</tr>
<tr>
<td>Claude 4</td>
<td><a href="https://www.anthropic.com/news/claude-4">Claude 4 Sonnet</a></td>
<td><a href="https://aws.amazon.com/bedrock/claude/">AWS Bedrock</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
</tr>
<tr>
<td>GPT</td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure#gpt-4">GPT-4 Turbo</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/overview">Azure OpenAI</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>GPT</td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure#gpt-4o-and-gpt-4-turbo">GPT-4o</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/overview">Azure OpenAI</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>GPT</td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models?tabs=python-secure#gpt-4o-and-gpt-4-turbo">GPT-4o-mini</a></td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/overview">Azure OpenAI</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>GPT</td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai&amp;tabs=global-standard%2Cstandard-chat-completions#gpt-5">GPT-5</a>)</td>
<td><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/overview">Azure OpenAI</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>GPT</td>
<td><a href="https://huggingface.co/openai/gpt-oss-120b">GPT-oss-120B</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>GPT</td>
<td><a href="https://huggingface.co/openai/gpt-oss-20b">GPT-oss-20B</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>Llama</td>
<td><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct">Llama 3 8B</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="dash-circle" &gt;}} Limited functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>Llama</td>
<td><a href="https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct">Llama 3.1 8B</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>Llama</td>
<td><a href="https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct">Llama 3 70B</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-dashed" &gt;}} Partial functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="dash-circle" &gt;}} Limited functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>Llama</td>
<td><a href="https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct">Llama 3.1 70B</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>Limited functionality</td>
</tr>
<tr>
<td>Llama</td>
<td><a href="https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct">Llama 3.3 70B</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>{{&lt; icon name="check-circle-filled" &gt;}} Full functionality</td>
<td>Limited functionality</td>
</tr>
</tbody>
</table>
<h3>Compatible models</h3>
<p>{{&lt; details &gt;}}</p>
<ul>
<li>Status: Beta</li>
</ul>
<p>{{&lt; /details &gt;}}</p>
<p>{{&lt; history &gt;}}</p>
<ul>
<li><a href="https://gitlab.com/groups/gitlab-org/-/epics/18556">Introduced</a> in GitLab 18.3 as a <a href="../../policy/development_stages_support.md#beta">beta</a>.</li>
</ul>
<p>{{&lt; /history &gt;}}</p>
<p>You can use your own compatible models and platform with GitLab Duo features. For compatible models not included in supported model families, use the general model family.</p>
<p>Compatible models are excluded from the definition of Customer Integrated Models in the <a href="https://handbook.gitlab.com/handbook/legal/ai-functionality-terms/">AI Functionality Terms</a>. Compatible models and platforms must adhere to the OpenAI API specification. Models and platforms that have
previously been marked as experimental or beta are now considered compatible models.</p>
<p>This feature is in beta and is therefore subject to change as we gather feedback and improve the integration:</p>
<ul>
<li>GitLab does not provide technical support for issues specific to your chosen model or platform.</li>
<li>Not all GitLab Duo features are guaranteed to work optimally with every compatible model.</li>
<li>Response quality, speed, and performance overall might vary significantly based on your model choice.</li>
</ul>
<table>
<thead>
<tr>
<th>Model family</th>
<th>Model requirements</th>
<th>Supported platforms</th>
</tr>
</thead>
<tbody>
<tr>
<td>General</td>
<td>Any model compatible with the <a href="https://platform.openai.com/docs/api-reference">OpenAI API specification</a></td>
<td>Any platform that provides OpenAI-compatible API endpoints</td>
</tr>
<tr>
<td>CodeGemma</td>
<td><a href="https://huggingface.co/google/codegemma-2b">CodeGemma 2b</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
<tr>
<td>CodeGemma</td>
<td><a href="https://huggingface.co/google/codegemma-7b-it">CodeGemma 7b-it</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
<tr>
<td>CodeGemma</td>
<td><a href="https://huggingface.co/google/codegemma-7b">CodeGemma 7b-code</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
<tr>
<td>Code Llama</td>
<td><a href="https://huggingface.co/meta-llama/CodeLlama-13b-Instruct-hf">Code-Llama 13b</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
<tr>
<td>DeepSeek Coder</td>
<td><a href="https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct">DeepSeek Coder 33b Instruct</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
<tr>
<td>DeepSeek Coder</td>
<td><a href="https://huggingface.co/deepseek-ai/deepseek-coder-33b-base">DeepSeek Coder 33b Base</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
<tr>
<td>Mistral</td>
<td><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2">Mistral 7B-it v0.2</a></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a> <br> <a href="https://aws.amazon.com/bedrock/mistral/">AWS Bedrock</a></td>
</tr>
<tr>
<td>Mistral</td>
<td><a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3">Mistral 7B-it v0.3</a> <sup>1</sup></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
<tr>
<td>Mistral</td>
<td><a href="https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1">Mixtral 8x7B-it v0.1</a> <sup>1</sup></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a>, <a href="https://aws.amazon.com/bedrock/mistral/">AWS Bedrock</a></td>
</tr>
<tr>
<td>Mistral</td>
<td><a href="https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1">Mixtral 8x22B-it v0.1</a> <sup>1</sup></td>
<td><a href="supported_llm_serving_platforms.md#for-self-hosted-model-deployments">vLLM</a></td>
</tr>
</tbody>
</table>
<p><strong>Footnotes</strong>:</p>
<ol>
<li>This model was <a href="../../update/deprecations.md#early-mistral-models-deprecated-for-gitlab-duo-self-hosted">deprecated</a> in GitLab 18.3. You should use Mistral Small 24B Instruct 2506 instead.</li>
</ol>
<!-- vale gitlab_base.Spelling = YES -->

<h2>GitLab AI vendor models</h2>
<p>{{&lt; details &gt;}}</p>
<ul>
<li>Status: Beta</li>
</ul>
<p>{{&lt; /details &gt;}}</p>
<p>{{&lt; history &gt;}}</p>
<ul>
<li><a href="https://gitlab.com/groups/gitlab-org/-/epics/17192">Introduced</a> in GitLab 18.3, with a <a href="../feature_flags/_index.html">feature flag</a> named <code>ai_self_hosted_vendored_features</code>. Disabled by default.</li>
<li><a href="https://gitlab.com/gitlab-org/gitlab/-/merge_requests/214030">Enabled by default</a> in GitLab 18.7</li>
</ul>
<p>{{&lt; /history &gt;}}</p>
<p>{{&lt; alert type="flag" &gt;}}</p>
<p>The availability of this feature is controlled by a feature flag.
For more information, see the history.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>GitLab AI vendor models integrate with GitLab-hosted AI Gateway infrastructure to provide access to AI models curated and made available by GitLab. Instead of using your own self-hosted models, you can choose to use GitLab AI vendor models for specific GitLab Duo features.</p>
<p>To choose which features use GitLab AI vendor models, see <a href="configure_duo_features.md#configure-a-feature-to-use-a-gitlab-ai-vendor-model">Configure GitLab AI vendor models</a>.</p>
<p>When enabled for a specific feature:</p>
<ul>
<li>All calls to those features configured with a GitLab AI vendor model use the GitLab-hosted AI Gateway, not the self-hosted AI Gateway.</li>
<li>No detailed logs are generated in the GitLab-hosted AI Gateway, even when <a href="logging.md#enable-logging">AI logs are enabled</a>. This prevents unintended leaks of sensitive information.</li>
</ul>
<h2>Hardware requirements</h2>
<p>The following hardware specifications are the minimum requirements for running GitLab Duo Self-Hosted on-premise. Requirements vary significantly based on the model size and intended usage:</p>
<h3>Base system requirements</h3>
<ul>
<li><strong>CPU</strong>:</li>
<li>Minimum: 8 cores (16 threads)</li>
<li>Recommended: 16+ cores for production environments</li>
<li><strong>RAM</strong>:</li>
<li>Minimum: 32 GB</li>
<li>Recommended: 64 GB for most models</li>
<li><strong>Storage</strong>:</li>
<li>SSD with sufficient space for model weights and data.</li>
</ul>
<h3>GPU requirements by model size</h3>
<table>
<thead>
<tr>
<th>Model size</th>
<th>Minimum GPU configuration</th>
<th>Minimum VRAM required</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B models<br>(for example, Mistral 7B)</td>
<td>1x NVIDIA A100 (40 GB)</td>
<td>35 GB</td>
</tr>
<tr>
<td>22B models<br>(for example, Codestral 22B)</td>
<td>2x NVIDIA A100 (80 GB)</td>
<td>110 GB</td>
</tr>
<tr>
<td>Mixtral 8x7B</td>
<td>2x NVIDIA A100 (80 GB)</td>
<td>220 GB</td>
</tr>
<tr>
<td>Mixtral 8x22B</td>
<td>8x NVIDIA A100 (80 GB)</td>
<td>526 GB</td>
</tr>
</tbody>
</table>
<p>Use <a href="https://huggingface.co/spaces/hf-accelerate/model-memory-usage">Hugging Face's memory utility</a> to verify memory requirements.</p>
<h3>Response time by model size and GPU</h3>
<h4>Small machine</h4>
<p>With a <code>a2-highgpu-2g</code> (2x NVIDIA A100 40 GB - 150 GB vRAM) or equivalent:</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>Number of requests</th>
<th>Average time per request (sec)</th>
<th>Average tokens in response</th>
<th>Average tokens per second per request</th>
<th>Total time for requests</th>
<th>Total TPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>1</td>
<td>7.09</td>
<td>717.0</td>
<td>101.19</td>
<td>7.09</td>
<td>101.17</td>
</tr>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>10</td>
<td>8.41</td>
<td>764.2</td>
<td>90.35</td>
<td>13.70</td>
<td>557.80</td>
</tr>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>100</td>
<td>13.97</td>
<td>693.23</td>
<td>49.17</td>
<td>20.81</td>
<td>3331.59</td>
</tr>
</tbody>
</table>
<h4>Medium machine</h4>
<p>With a <code>a2-ultragpu-4g</code> (4x NVIDIA A100 40 GB - 340 GB vRAM) machine on GCP or equivalent:</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>Number of requests</th>
<th>Average time per request (sec)</th>
<th>Average tokens in response</th>
<th>Average tokens per second per request</th>
<th>Total time for requests</th>
<th>Total TPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>1</td>
<td>3.80</td>
<td>499.0</td>
<td>131.25</td>
<td>3.80</td>
<td>131.23</td>
</tr>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>10</td>
<td>6.00</td>
<td>740.6</td>
<td>122.85</td>
<td>8.19</td>
<td>904.22</td>
</tr>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>100</td>
<td>11.71</td>
<td>695.71</td>
<td>59.06</td>
<td>15.54</td>
<td>4477.34</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct-v0.1</td>
<td>1</td>
<td>6.50</td>
<td>400.0</td>
<td>61.55</td>
<td>6.50</td>
<td>61.53</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct-v0.1</td>
<td>10</td>
<td>16.58</td>
<td>768.9</td>
<td>40.33</td>
<td>32.56</td>
<td>236.13</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct-v0.1</td>
<td>100</td>
<td>25.90</td>
<td>767.38</td>
<td>26.87</td>
<td>55.57</td>
<td>1380.68</td>
</tr>
</tbody>
</table>
<h4>Large machine</h4>
<p>With a <code>a2-ultragpu-8g</code> (8 x NVIDIA A100 80 GB - 1360 GB vRAM) machine on GCP or equivalent:</p>
<table>
<thead>
<tr>
<th>Model name</th>
<th>Number of requests</th>
<th>Average time per request (sec)</th>
<th>Average tokens in response</th>
<th>Average tokens per second per request</th>
<th>Total time for requests (sec)</th>
<th>Total TPS</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>1</td>
<td>3.23</td>
<td>479.0</td>
<td>148.41</td>
<td>3.22</td>
<td>148.36</td>
</tr>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>10</td>
<td>4.95</td>
<td>678.3</td>
<td>135.98</td>
<td>6.85</td>
<td>989.11</td>
</tr>
<tr>
<td>Mistral-7B-Instruct-v0.3</td>
<td>100</td>
<td>10.14</td>
<td>713.27</td>
<td>69.63</td>
<td>13.96</td>
<td>5108.75</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct-v0.1</td>
<td>1</td>
<td>6.08</td>
<td>709.0</td>
<td>116.69</td>
<td>6.07</td>
<td>116.64</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct-v0.1</td>
<td>10</td>
<td>9.95</td>
<td>645.0</td>
<td>63.68</td>
<td>13.40</td>
<td>481.06</td>
</tr>
<tr>
<td>Mixtral-8x7B-Instruct-v0.1</td>
<td>100</td>
<td>13.83</td>
<td>585.01</td>
<td>41.80</td>
<td>20.38</td>
<td>2869.12</td>
</tr>
<tr>
<td>Mixtral-8x22B-Instruct-v0.1</td>
<td>1</td>
<td>14.39</td>
<td>828.0</td>
<td>57.56</td>
<td>14.38</td>
<td>57.55</td>
</tr>
<tr>
<td>Mixtral-8x22B-Instruct-v0.1</td>
<td>10</td>
<td>20.57</td>
<td>629.7</td>
<td>30.24</td>
<td>28.02</td>
<td>224.71</td>
</tr>
<tr>
<td>Mixtral-8x22B-Instruct-v0.1</td>
<td>100</td>
<td>27.58</td>
<td>592.49</td>
<td>21.34</td>
<td>36.80</td>
<td>1609.85</td>
</tr>
</tbody>
</table>
<h3>AI Gateway Hardware Requirements</h3>
<p>For recommendations on AI Gateway hardware, see the <a href="../../install/install_ai_gateway.md#scaling-recommendations">AI Gateway scaling recommendations</a>.</p></code></pre>
</body>
</html>
