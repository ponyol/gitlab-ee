<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title># Enable logging</title>
    <link rel="stylesheet" href="/../../style.css">
</head>
<body>
    <pre><code><hr />
<p>stage: AI-powered
group: Custom Models
info: To determine the technical writer assigned to the Stage/Group associated with this page, see https://handbook.gitlab.com/handbook/product/ux/technical-writing/#assignments
description: Enable logging for self-hosted models.
title: Enable logging for self-hosted models</p>
<hr />
<p>{{&lt; details &gt;}}</p>
<ul>
<li>Tier: Premium, Ultimate</li>
<li>Add-on: GitLab Duo Enterprise</li>
<li>Offering: GitLab Self-Managed</li>
</ul>
<p>{{&lt; /details &gt;}}</p>
<p>{{&lt; history &gt;}}</p>
<ul>
<li><a href="https://gitlab.com/groups/gitlab-org/-/epics/12972">Introduced</a> in GitLab 17.1 <a href="../feature_flags/_index.html">with a flag</a> named <code>ai_custom_model</code>. Disabled by default.</li>
<li><a href="https://gitlab.com/groups/gitlab-org/-/epics/15176">Enabled on GitLab Self-Managed</a> in GitLab 17.6.</li>
<li>Changed to require GitLab Duo add-on in GitLab 17.6 and later.</li>
<li>Feature flag <code>ai_custom_model</code> removed in GitLab 17.8.</li>
<li>Generally available in GitLab 17.9.</li>
<li>Ability to turn logging on and off through the UI added in GitLab 17.9.</li>
<li>Changed to include Premium in GitLab 18.0.</li>
</ul>
<p>{{&lt; /history &gt;}}</p>
<p>Monitor your self-hosted model performance and debug issues more effectively with detailed
logging for GitLab Duo Self-Hosted.</p>
<h2>Enable logging</h2>
<p>Prerequisites:</p>
<ul>
<li>You must be an administrator.</li>
</ul>
<p>To enable logging:</p>
<ol>
<li>In the upper-right corner, select <strong>Admin</strong>.</li>
<li>On the left sidebar, select <strong>GitLab Duo</strong>.</li>
<li>Select <strong>Change configuration</strong>.</li>
<li>Under <strong>Enable AI logs</strong>, select <strong>Capture detailed information about AI-related activities and requests</strong>.</li>
<li>Select <strong>Save changes</strong>.</li>
</ol>
<p>You can now access the logs in your GitLab installation.</p>
<h2>Logs in your GitLab installation</h2>
<p>The logging setup is designed to protect sensitive information while maintaining transparency about system operations, and is made up of the following components:</p>
<ul>
<li>Logs that capture requests to the GitLab instance.</li>
<li>Logging control.</li>
<li>The <code>llm.log</code> file.</li>
</ul>
<h3>Logs that capture requests to the GitLab instance</h3>
<p>Logging in the <code>application.json</code>, <code>production_json.log</code>, and <code>production.log</code> files, among others, capture requests to the GitLab instance:</p>
<ul>
<li><strong>Filtered Requests</strong>: We log the requests in these files but ensure that sensitive data (such as input parameters) is <strong>filtered</strong>. This means that while the request metadata is captured (for example, the request type, endpoint, and response status), the actual input data (for example, the query parameters, variables, and content) is not logged to prevent the exposure of sensitive information.</li>
<li><strong>Example 1</strong>: In the case of a code suggestions completion request, the logs capture the request details while filtering sensitive information:</li>
</ul>
<p><code>json
  {
    "method": "POST",
    "path": "/api/graphql",
    "controller": "GraphqlController",
    "action": "execute",
    "status": 500,
    "params": [
      {"key": "query", "value": "[FILTERED]"},
      {"key": "variables", "value": "[FILTERED]"},
      {"key": "operationName", "value": "chat"}
    ],
    "exception": {
      "class": "NoMethodError",
      "message": "undefined method `id` for {:skip=&gt;true}:Hash"
    },
    "time": "2024-08-28T14:13:50.328Z"
  }</code></p>
<p>As shown, while the error information and general structure of the request are logged, the sensitive input parameters are marked as <code>[FILTERED]</code>.</p>
<ul>
<li><strong>Example 2</strong>: In the case of a code suggestions completion request, the logs also capture the request details while filtering sensitive information:</li>
</ul>
<p><code>json
  {
    "method": "POST",
    "path": "/api/v4/code_suggestions/completions",
    "status": 200,
    "params": [
      {"key": "prompt_version", "value": 1},
      {"key": "current_file", "value": {"file_name": "/test.rb", "language_identifier": "ruby", "content_above_cursor": "[FILTERED]", "content_below_cursor": "[FILTERED]"}},
      {"key": "telemetry", "value": []}
    ],
    "time": "2024-10-15T06:51:09.004Z"
  }</code></p>
<p>As shown, while the general structure of the request is logged, the sensitive input parameters such as <code>content_above_cursor</code> and <code>content_below_cursor</code> are marked as <code>[FILTERED]</code>.</p>
<h3>Logging Control</h3>
<p>You control a subset of these logs by turning <a href="#enable-logging">AI Logs</a> on and off through the GitLab Duo settings page. Turning AI logs off disables logging for specific operations.</p>
<h3>The <code>llm.log</code> file</h3>
<p>When <a href="#enable-logging">AI Logs</a> are enabled, code generation and Chat events that occur through your GitLab Self-Managed instance are captured in the <a href="../logs/_index.md#llmlog"><code>llm.log</code> file</a>. The log file does not capture anything when it is not enabled. Code completion logs are captured directly in the AI Gateway. These logs are not transmitted to GitLab, and are only visible on your GitLab Self-Managed infrastructure.</p>
<ul>
<li><a href="../logs/_index.html">Rotate, manage, export, and visualize the logs in <code>llm.log</code></a>.</li>
<li><a href="../logs/_index.md#llm-input-and-output-logging">View the log file location (for example, so you can delete logs)</a>.</li>
</ul>
<h3>Logs in your AI Gateway container</h3>
<p>To specify the location of logs generated by AI Gateway and the GitLab Duo Agent Platform, run:</p>
<pre><code class="language-shell">docker run -e AIGW_GITLAB_URL=&lt;your_gitlab_instance&gt; \
 -e AIGW_GITLAB_API_URL=https://&lt;your_gitlab_domain&gt;/api/v4/ \
 -e DUO_WORKFLOW_SELF_SIGNED_JWT__SIGNING_KEY=&quot;your-signing-key&quot; \
 -e AIGW_LOGGING__TO_FILE=&quot;aigateway.log&quot; \
 -e DUO_WORKFLOW_LOGGING__TO_FILE=&quot;duo_agent_platform.log&quot; \
 -v &lt;your_aigateway_file_path&gt;:aigateway.log \
 -v &lt;your_duo_agent_platform_file_path&gt;:duo_agent_platform.log \
 &lt;image&gt;
</code></pre>
<p>By default, the logging level is set to <code>INFO</code>. To change the logging level to <code>DEBUG</code>, run:</p>
<pre><code class="language-shell">docker run -e AIGW_GITLAB_URL=&lt;your_gitlab_instance&gt; \
 -e AIGW_GITLAB_API_URL=https://&lt;your_gitlab_domain&gt;/api/v4/ \
 -e DUO_WORKFLOW_SELF_SIGNED_JWT__SIGNING_KEY=&quot;your-signing-key&quot; \
 -e AIGW_LOGGING__TO_FILE=&quot;aigateway.log&quot; \
 -e DUO_WORKFLOW_LOGGING__TO_FILE=&quot;duo_agent_platform.log&quot; \
 -e AIGW_LOGGING__LEVEL=&quot;DEBUG&quot; \
 -e DUO_WORKFLOW_LOGGING__LEVEL=&quot;DEBUG&quot; \
 -v &lt;your_aigateway_file_path&gt;:aigateway.log \
 -v &lt;your_duo_agent_platform_file_path&gt;:duo_agent_platform.log \
 &lt;image&gt;
</code></pre>
<p>Additionally, to log all of the debug statements from <code>litellm</code>, add the following environment variables:</p>
<pre><code class="language-shell">-e AIGW_LOGGING__ENABLE_LITELLM_LOGGING=true
</code></pre>
<p>If you do not specify a filename, logs are streamed to the output and can also be managed using Docker logs.
For more information, see the <a href="https://docs.docker.com/reference/cli/docker/container/logs/">Docker Logs documentation</a>.</p>
<p>Additionally, the outputs of the AI Gateway execution can help with debugging issues. To access them:</p>
<ul>
<li>When using Docker:</li>
</ul>
<p><code>shell
  docker logs &lt;container-id&gt;</code></p>
<ul>
<li>When using Kubernetes:</li>
</ul>
<p><code>shell
  kubectl logs &lt;container-name&gt;</code></p>
<p>To ingest these logs into the logging solution, see your logging provider documentation.</p>
<h3>Logs structure</h3>
<p>When a POST request is made (for example, to the <code>/chat/completions</code> endpoint), the server logs the request:</p>
<ul>
<li>Payload</li>
<li>Headers</li>
<li>Metadata</li>
</ul>
<h4>1. Request payload</h4>
<p>The JSON payload typically includes the following fields:</p>
<ul>
<li><code>messages</code>: An array of message objects.</li>
<li>Each message object contains:<ul>
<li><code>content</code>: A string representing the user's input or query.</li>
<li><code>role</code>: Indicates the role of the message sender (for example, <code>user</code>).</li>
</ul>
</li>
<li><code>model</code>: A string specifying the model to be used (for example, <code>mistral</code>).</li>
<li><code>max_tokens</code>: An integer specifying the maximum number of tokens to generate in the response.</li>
<li><code>n</code>: An integer indicating the number of completions to generate.</li>
<li><code>stop</code>: An array of strings denoting stop sequences for the generated text.</li>
<li><code>stream</code>: A boolean indicating whether the response should be streamed.</li>
<li><code>temperature</code>: A float controlling the randomness of the output.</li>
</ul>
<h5>Example request</h5>
<pre><code class="language-json">{
    &quot;messages&quot;: [
        {
            &quot;content&quot;: &quot;&lt;s&gt;[SUFFIX]None[PREFIX]# # build a hello world ruby method\n def say_goodbye\n    puts \&quot;Goodbye, World!\&quot;\n  end\n\ndef main\n  say_hello\n  say_goodbye\nend\n\nmain&quot;,
            &quot;role&quot;: &quot;user&quot;
        }
    ],
    &quot;model&quot;: &quot;mistral&quot;,
    &quot;max_tokens&quot;: 128,
    &quot;n&quot;: 1,
    &quot;stop&quot;: [&quot;[INST]&quot;, &quot;[/INST]&quot;, &quot;[PREFIX]&quot;, &quot;[MIDDLE]&quot;, &quot;[SUFFIX]&quot;],
    &quot;stream&quot;: false,
    &quot;temperature&quot;: 0.0
}
</code></pre>
<h4>2. Request headers</h4>
<p>The request headers provide additional context about the client making the request. Key headers might include:</p>
<ul>
<li><code>Authorization</code>: Contains the Bearer token for API access.</li>
<li><code>Content-Type</code>: Indicates the media type of the resource (for example, <code>JSON</code>).</li>
<li><code>User-Agent</code>: Information about the client software making the request.</li>
<li><code>X-Stainless-</code> headers: Various headers providing additional metadata about the client environment.</li>
</ul>
<h5>Example request headers</h5>
<pre><code class="language-json">{
    &quot;host&quot;: &quot;0.0.0.0:4000&quot;,
    &quot;accept-encoding&quot;: &quot;gzip, deflate&quot;,
    &quot;connection&quot;: &quot;keep-alive&quot;,
    &quot;accept&quot;: &quot;application/json&quot;,
    &quot;content-type&quot;: &quot;application/json&quot;,
    &quot;user-agent&quot;: &quot;AsyncOpenAI/Python 1.51.0&quot;,
    &quot;authorization&quot;: &quot;Bearer &lt;TOKEN&gt;&quot;,
    &quot;content-length&quot;: &quot;364&quot;
}
</code></pre>
<h4>3. Request metadata</h4>
<p>The metadata includes various fields that describe the context of the request:</p>
<ul>
<li><code>requester_metadata</code>: Additional metadata about the requester.</li>
<li><code>user_api_key</code>: The API key used for the request (anonymized).</li>
<li><code>api_version</code>: The version of the API being used.</li>
<li><code>request_timeout</code>: The timeout duration for the request.</li>
<li><code>call_id</code>: A unique identifier for the call.</li>
</ul>
<h5>Example metadata</h5>
<pre><code class="language-json">{
    &quot;user_api_key&quot;: &quot;&lt;ANONYMIZED_KEY&gt;&quot;,
    &quot;api_version&quot;: &quot;1.48.18&quot;,
    &quot;request_timeout&quot;: 600,
    &quot;call_id&quot;: &quot;e1aaa316-221c-498c-96ce-5bc1e7cb63af&quot;
}
</code></pre>
<h3>Example response</h3>
<p>The server responds with a structured model response. For example:</p>
<pre><code class="language-python">Response: ModelResponse(
    id='chatcmpl-5d16ad41-c130-4e33-a71e-1c392741bcb9',
    choices=[
        Choices(
            finish_reason='stop',
            index=0,
            message=Message(
                content=' Here is the corrected Ruby code for your function:\n\n```ruby\ndef say_hello\n  puts &quot;Hello, World!&quot;\nend\n\ndef say_goodbye\n    puts &quot;Goodbye, World!&quot;\nend\n\ndef main\n  say_hello\n  say_goodbye\nend\n\nmain\n```\n\nIn your original code, the method names were misspelled as `say_hell` and `say_gobdye`. I corrected them to `say_hello` and `say_goodbye`. Also, there was no need for the prefix',
                role='assistant',
                tool_calls=None,
                function_call=None
            )
        )
    ],
    created=1728983827,
    model='mistral',
    object='chat.completion',
    system_fingerprint=None,
    usage=Usage(
        completion_tokens=128,
        prompt_tokens=69,
        total_tokens=197,
        completion_tokens_details=None,
        prompt_tokens_details=None
    )
)
</code></pre>
<h3>Logs in your inference service provider</h3>
<p>GitLab does not manage logs generated by your inference service provider. See the documentation of your inference service
provider on how to use their logs.</p>
<h2>Logging behavior in GitLab and AI Gateway environments</h2>
<p>GitLab provides logging functionality for AI-related activities through the use of <code>llm.log</code>, which captures inputs, outputs, and other relevant information. However, the logging behavior differs depending on whether the GitLab instance and AI Gateway are <strong>self-hosted</strong> or <strong>cloud-connected</strong>.</p>
<p>By default, the log does not contain LLM prompt input and response output to support <a href="../../user/gitlab_duo/data_usage.md#data-retention">data retention policies</a> of AI feature data.</p>
<h2>Logging Scenarios</h2>
<h3>GitLab Self-Managed and self-hosted AI Gateway</h3>
<p>In this configuration, both GitLab and the AI Gateway are hosted by the customer.</p>
<ul>
<li><strong>Logging Behavior</strong>: Full logging is enabled, and all prompts, inputs, and outputs are logged to <code>llm.log</code> on the instance.</li>
<li>When <a href="#enable-logging">AI logs</a> are enabled, extra debugging information is logged, including:</li>
<li>Preprocessed prompts.</li>
<li>Final prompts.</li>
<li>Additional context.</li>
<li><strong>Privacy</strong>: Because both GitLab and the AI Gateway are self-hosted:</li>
<li>The customer has full control over data handling.</li>
<li>Logging of sensitive information can be enabled or disabled at the customer's discretion.</li>
</ul>
<p>{{&lt; alert type="note" &gt;}}</p>
<p>When an AI feature uses a GitLab AI third-party vendor model, no detailed logs are generated in the GitLab-hosted AI Gateway, even when <a href="#enable-logging">AI logs are enabled</a>. This prevents unintended leaks of sensitive information.</p>
<p>{{&lt; /alert &gt;}}</p>
<h3>GitLab Self-Managed and GitLab-managed AI Gateway (cloud-connected)</h3>
<p>In this scenario, the customer hosts GitLab but relies on the GitLab-managed AI Gateway for AI processing.</p>
<ul>
<li><strong>Logging Behavior</strong>: Prompts and inputs sent to the AI Gateway are <strong>not logged</strong> in the cloud-connected AI Gateway to prevent exposure of sensitive information such as personally identifiable information (PII).</li>
<li><strong>Expanded Logging</strong>: Even if <a href="#enable-logging">AI logs are enabled</a>, no detailed logs are generated in the GitLab-managed AI Gateway to avoid unintended leaks of sensitive information.</li>
<li>Logging remains <strong>minimal</strong> in this setup, and the expanded logging features are disabled by default.</li>
<li><strong>Privacy</strong>: This configuration is designed to ensure that sensitive data is not logged in a cloud environment.</li>
</ul>
<h2>AI logs</h2>
<p>The <a href="#enable-logging">AI logs</a> control whether additional debugging information, including prompts and inputs, is logged. This configuration is essential for monitoring and debugging AI-related activities.</p>
<h3>Behavior by Deployment Setup</h3>
<ul>
<li><strong>GitLab Self-Managed and self-hosted AI Gateway</strong>:</li>
<li>The setting enables detailed logging to <code>llm.log</code> on both the self-hosted instance and the AI Gateway, capturing inputs and outputs for AI models.</li>
<li>Logging remains disabled for the cloud-connected AI Gateway to protect sensitive data, even when a feature uses a GitLab third-party vendor model.</li>
<li><strong>GitLab Self-Managed and GitLab-managed AI Gateway</strong>:</li>
<li>The setting enables detailed logging to <code>llm.log</code> on your GitLab Self-Managed instance.</li>
<li>The setting does <strong>not</strong> activate expanded logging for the GitLab-managed AI Gateway. Logging remains disabled for the cloud-connected AI Gateway to protect sensitive data.</li>
</ul>
<h3>Logging in cloud-connected AI Gateways</h3>
<p>To prevent potential data leakage of sensitive information, expanded logging (including prompts and inputs) is intentionally disabled when using a cloud-connected AI Gateway. Preventing the exposure of PII is a priority.</p>
<h3>Cross-referencing logs between the AI Gateway and GitLab</h3>
<p>The property <code>correlation_id</code> is assigned to every request and is carried across different components that respond to a
request. For more information, see the <a href="../logs/tracing_correlation_id.html">documentation on finding logs with a correlation ID</a>.</p>
<p>The Correlation ID can be found in your AI Gateway and GitLab logs. However, it is not present in your model provider logs.</p>
<h4>Related topics</h4>
<ul>
<li><a href="../logs/log_parsing.html">Parsing GitLab logs with jq</a></li>
<li><a href="../logs/tracing_correlation_id.md#searching-your-logs-for-the-correlation-id">Searching your logs for the correlation ID</a></li>
</ul></code></pre>
</body>
</html>
