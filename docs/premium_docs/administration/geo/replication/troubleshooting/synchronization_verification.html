<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title># Diagnostic procedures</title>
    <link rel="stylesheet" href="/../../../../style.css">
</head>
<body>
    <pre><code><hr />
<p>stage: Tenant Scale
group: Geo
info: To determine the technical writer assigned to the Stage/Group associated with this page, see https://handbook.gitlab.com/handbook/product/ux/technical-writing/#assignments
title: Troubleshooting Geo synchronization and verification errors
description: "Troubleshoot Geo synchronization and verification failures, covering manual retry procedures, bulk operations, error diagnosis, and data consistency restoration."</p>
<hr />
<p>{{&lt; details &gt;}}</p>
<ul>
<li>Tier: Premium, Ultimate</li>
<li>Offering: GitLab Self-Managed</li>
</ul>
<p>{{&lt; /details &gt;}}</p>
<p>If you notice replication or verification failures in <code>Admin &gt; Geo &gt; Sites</code> or the <a href="common.md#sync-status-rake-task">Sync status Rake task</a>, you can try to resolve the failures with the following general steps:</p>
<ol>
<li>Geo automatically retries failures. If the failures are new and few in number, or if you suspect the root cause is already resolved, then you can wait to see if the failures go away.</li>
<li>If failures were present for a long time, then many retries have already occurred, and the interval between automatic retries has increased to up to 4 hours depending on the type of failure. If you suspect the root cause is already resolved, you can <a href="#manually-retry-replication-or-verification">manually retry replication or verification</a> to avoid the wait.</li>
<li>If the failures persist, use the following sections to try to resolve them.</li>
</ol>
<h2>Diagnostic procedures</h2>
<p>Before attempting manual retries, you can use these enhanced diagnostic procedures to better understand the scope and nature of synchronization issues.</p>
<h3>Registry status check</h3>
<p>This procedure provides detailed status information for all Geo registry types and helps identify patterns in failures.</p>
<ol>
<li>
<p><a href="../../../operations/rails_console.md#starting-a-rails-console-session">Start a Rails console session</a> on the <strong>secondary</strong> site.</p>
</li>
<li>
<p>Run the following script to get a comprehensive overview:</p>
</li>
</ol>
<p>```ruby
   def output_geo_failures()
     registry_classes = [
       Geo::UploadRegistry,
       Geo::JobArtifactRegistry,
       Geo::PackageFileRegistry,
       Geo::PagesDeploymentRegistry,
       Geo::ProjectRepositoryRegistry,
       Geo::TerraformStateVersionRegistry,
       Geo::MergeRequestDiffRegistry,
       Geo::LfsObjectRegistry,
       Geo::PipelineArtifactRegistry,
       Geo::CiSecureFileRegistry
     ]</p>
<pre><code> registry_classes.each do |klass|
   puts "\n=== #{klass.name} ==="
   puts "Total: #{klass.count}"
   puts "Failed: #{klass.failed.count}"
   puts "Synced: #{klass.synced.count}"
   puts "Pending: #{klass.pending.count}"
   puts "Started: #{klass.with_state(:started).count}"

   if klass.failed.count &gt; 0
      puts "\nSample failed records:"
      klass.failed.limit(3).each { |record| puts "  ID: #{record.id}, Error: #{record.last_sync_failure}" }
   end
 end

 nil
</code></pre>
<p>end</p>
<p>output_geo_failures()
   ```</p>
<ol>
<li>This script outputs detailed information for each registry type, including:</li>
<li>Total count of records</li>
<li>Number of failed, synced, and pending records</li>
<li>Sample failed records for investigation</li>
</ol>
<h2>Manually retry replication or verification</h2>
<p>In <a href="../../../operations/rails_console.md#starting-a-rails-console-session">Rails console</a> in a
secondary Geo site, you can:</p>
<ul>
<li><a href="#resync-and-reverify-individual-components">Manually resync and reverify individual components</a></li>
<li><a href="#resync-and-reverify-multiple-components">Manually resync and reverify multiple components</a></li>
</ul>
<h3>Resync and reverify individual components</h3>
<p>On the secondary site, visit <strong>Admin</strong> &gt; <strong>Geo</strong> &gt; <strong>Replication</strong> to force a resync or reverify of individual items.</p>
<p>However, if this doesn't work, you can perform the same action using the Rails console. The
following sections describe how to use internal application commands in the
<a href="../../../operations/rails_console.md#starting-a-rails-console-session">Rails console</a> to cause
replication or verification for individual records synchronously or asynchronously.</p>
<h4>Obtaining a Replicator instance</h4>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>Commands that change data can cause damage if not run correctly or under the right conditions.
Always run commands in a test environment first and have a backup instance ready to restore.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>Before you can perform any sync or verify operations, you need to obtain a Replicator instance.</p>
<p>First, <a href="../../../operations/rails_console.md#starting-a-rails-console-session">start a Rails console session</a>
in a <strong>primary</strong> or <strong>secondary</strong> site, depending on what you want to do.</p>
<p><strong>Primary</strong> site:</p>
<ul>
<li>You can checksum a resource</li>
</ul>
<p><strong>Secondary</strong> site:</p>
<ul>
<li>You can sync a resource</li>
<li>You can checksum a resource and verify that checksum against the primary site's checksum</li>
</ul>
<p>Next, run one of the following snippets to get a Replicator instance.</p>
<h5>Given a model record's ID</h5>
<ul>
<li>Replace <code>123</code> with the actual ID.</li>
<li>Replace <code>Packages::PackageFile</code> with any of the
  <a href="#geo-data-type-model-classes">Geo data type Model classes</a>.</li>
</ul>
<pre><code class="language-ruby">model_record = Packages::PackageFile.find_by(id: 123)
replicator = model_record.replicator
</code></pre>
<h5>Given a registry record's ID</h5>
<ul>
<li>Replace <code>432</code> with the actual ID. A Registry record may or may not have the same ID
  value as the Model record that it tracks.</li>
<li>Replace <code>Geo::PackageFileRegistry</code> with any of the <a href="#geo-registry-classes">Geo Registry classes</a>.</li>
</ul>
<p>In a secondary Geo site:</p>
<pre><code class="language-ruby">registry_record = Geo::PackageFileRegistry.find_by(id: 432)
replicator = registry_record.replicator
</code></pre>
<h5>Given an error message in a Registry record's <code>last_sync_failure</code></h5>
<ul>
<li>Replace <code>Geo::PackageFileRegistry</code> with any of the <a href="#geo-registry-classes">Geo Registry classes</a>.</li>
<li>Replace <code>error message here</code> with the actual error message.</li>
</ul>
<pre><code class="language-ruby">registry = Geo::PackageFileRegistry.find_by(&quot;last_sync_failure LIKE '%error message here%'&quot;)
replicator = registry.replicator
</code></pre>
<h5>Given an error message in a Registry record's <code>verification_failure</code></h5>
<ul>
<li>Replace <code>Geo::PackageFileRegistry</code> with any of the <a href="#geo-registry-classes">Geo Registry classes</a>.</li>
<li>Replace <code>error message here</code> with the actual error message.</li>
</ul>
<pre><code class="language-ruby">registry = Geo::PackageFileRegistry.find_by(&quot;verification_failure LIKE '%error message here%'&quot;)
replicator = registry.replicator
</code></pre>
<h4>Performing operations with a Replicator instance</h4>
<p>After you have a Replicator instance stored in a <code>replicator</code> variable, you can perform many
operations:</p>
<h5>Sync in the console</h5>
<p>This snippet only works in a <strong>secondary</strong> site.</p>
<p>This executes the sync code synchronously in the console, so you can observe how long it takes to
sync a resource, or view a full error backtrace.</p>
<pre><code class="language-ruby">replicator.sync
</code></pre>
<p>Optionally, make the log level of the console more verbose than the configured log level, and then
perform a sync:</p>
<pre><code class="language-ruby">Rails.logger.level = :debug
</code></pre>
<h5>Checksum or verify in the console</h5>
<p>This snippet works in any <strong>primary</strong> or <strong>secondary</strong> site.</p>
<p>In a <strong>primary</strong> site, it checksums the resource and stores the result in the main GitLab
database. In a <strong>secondary</strong> site, it checksums the resource, compares it against the checksum in
the main GitLab database (generated by the <strong>primary</strong> site), and stores the result in the Geo
Tracking database.</p>
<p>This executes the checksum and verification code synchronously in the console, so you can observe
how long it takes, or view a full error backtrace.</p>
<pre><code class="language-ruby">replicator.verify
</code></pre>
<h5>Sync in a Sidekiq job</h5>
<p>This snippet only works in a <strong>secondary</strong> site.</p>
<p>It enqueues a job for Sidekiq to perform a <a href="#sync-in-the-console">sync</a> of the resource.</p>
<pre><code class="language-ruby">replicator.enqueue_sync
</code></pre>
<h5>Verify in a Sidekiq job</h5>
<p>This snippet works in any <strong>primary</strong> or <strong>secondary</strong> site.</p>
<p>It enqueues a job for Sidekiq to perform a
<a href="#checksum-or-verify-in-the-console">checksum or verify</a> of the resource.</p>
<pre><code class="language-ruby">replicator.verify_async
</code></pre>
<h5>Get a model record</h5>
<p>This snippet works in any <strong>primary</strong> or <strong>secondary</strong> site.</p>
<pre><code class="language-ruby">replicator.model_record
</code></pre>
<h5>Get a registry record</h5>
<p>This snippet only works in a <strong>secondary</strong> site because registry tables are stored in the Geo
Tracking DB.</p>
<pre><code class="language-ruby">replicator.registry
</code></pre>
<h4>Geo data type Model classes</h4>
<p>A Geo data type is a specific class of data that is required by one or more GitLab features to store
relevant data and is replicated by Geo to secondary sites.</p>
<ul>
<li><strong>Blob types</strong>:</li>
<li><code>Ci::JobArtifact</code></li>
<li><code>Ci::PipelineArtifact</code></li>
<li><code>Ci::SecureFile</code></li>
<li><code>LfsObject</code></li>
<li><code>MergeRequestDiff</code></li>
<li><code>Packages::PackageFile</code></li>
<li><code>PagesDeployment</code></li>
<li><code>Terraform::StateVersion</code></li>
<li><code>Upload</code></li>
<li><code>DependencyProxy::Manifest</code></li>
<li><code>DependencyProxy::Blob</code></li>
<li><strong>Git Repository types</strong>:</li>
<li><code>DesignManagement::Repository</code></li>
<li><code>ProjectRepository</code></li>
<li><code>ProjectWikiRepository</code></li>
<li><code>SnippetRepository</code></li>
<li><code>GroupWikiRepository</code></li>
<li><strong>Other types</strong>:</li>
<li><code>ContainerRepository</code></li>
</ul>
<p>The main kinds of classes are Registry, Model, and Replicator. If you have an instance of one of
these classes, you can get the others. The Registry and Model mostly manage PostgreSQL DB state. The
Replicator knows how to replicate or verify the non-PostgreSQL data (file/Git repository/Container
repository).</p>
<h4>Geo Registry classes</h4>
<p>In the context of GitLab Geo, a <strong>registry record</strong> refers to registry tables in
the Geo tracking database. Each record tracks a single replicable in the main
GitLab database, such as an LFS file, or a project Git repository. The Rails
models that correspond to Geo registry tables that can be queried are:</p>
<ul>
<li><strong>Blob types</strong>:</li>
<li><code>Geo::CiSecureFileRegistry</code></li>
<li><code>Geo::DependencyProxyBlobRegistry</code></li>
<li><code>Geo::DependencyProxyManifestRegistry</code></li>
<li><code>Geo::JobArtifactRegistry</code></li>
<li><code>Geo::LfsObjectRegistry</code></li>
<li><code>Geo::MergeRequestDiffRegistry</code></li>
<li><code>Geo::PackageFileRegistry</code></li>
<li><code>Geo::PagesDeploymentRegistry</code></li>
<li><code>Geo::PipelineArtifactRegistry</code></li>
<li><code>Geo::ProjectWikiRepositoryRegistry</code></li>
<li><code>Geo::SnippetRepositoryRegistry</code></li>
<li><code>Geo::TerraformStateVersionRegistry</code></li>
<li><code>Geo::UploadRegistry</code></li>
<li><strong>Git Repository types</strong>:</li>
<li><code>Geo::DesignManagementRepositoryRegistry</code></li>
<li><code>Geo::ProjectRepositoryRegistry</code></li>
<li><code>Geo::ProjectWikiRepositoryRegistry</code></li>
<li><code>Geo::SnippetRepositoryRegistry</code></li>
<li><code>Geo::GroupWikiRepositoryRegistry</code></li>
<li><strong>Other types</strong>:</li>
<li><code>Geo::ContainerRepositoryRegistry</code></li>
</ul>
<h3>Resync and reverify multiple components</h3>
<p>{{&lt; history &gt;}}</p>
<ul>
<li>Bulk resync and reverify <a href="https://gitlab.com/gitlab-org/gitlab/-/issues/364729">added</a> in GitLab 16.5.</li>
</ul>
<p>{{&lt; /history &gt;}}</p>
<p>When component resources fail to sync or verify, you can trigger bulk actions to re-kick the replication queue.
These actions reset the retry count and schedule time back to 0, causing the system to process the failed resources
sooner rather than waiting up to 1 hour.</p>
<p>{{&lt; alert type="note" &gt;}}</p>
<p>These actions don't immediately process the resources. Instead, they re-queue the background jobs that
handle synchronization and verification. The actual replication work happens asynchronously through the standard Geo
replication process.</p>
<p>{{&lt; /alert &gt;}}</p>
<h4>How resync and reverification works</h4>
<p>When you trigger a resync or reverification action, the system marks matching records as <code>pending</code>. The Geo resync and
reverification background workers pick up these records and process them according to normal queue priority.
This mechanism allows you to expedite the processing of failed resources without immediately blocking on the operation.</p>
<p>{{&lt; alert type="note" &gt;}}</p>
<p>It is not possible to reverify a record which is not successfully synced. Only a synced record can be verified.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>It is possible to trigger bulk actions from the UI or from the Rails console.</p>
<h4>From the UI</h4>
<p>You can schedule a full resync of all resources of one component from the UI:</p>
<ol>
<li>In the upper-right corner, select <strong>Admin</strong>.</li>
<li>Select <strong>Geo</strong> &gt; <strong>Sites</strong>.</li>
<li>Under <strong>Replication details</strong>, select the desired component.</li>
</ol>
<h5>Resync resources for the selected component</h5>
<ol>
<li>Select <strong>Resync all</strong>: this resets the status of all records for the selected resource, regardless of whether they are already synced or not.</li>
<li>Select <strong>Resync all failed</strong>: this resets all records for which sync failed.</li>
</ol>
<h5>Reverify resources for the selected component</h5>
<ol>
<li>Select <strong>Reverify all</strong>: this resets the status of all records for the selected resource, regardless of whether they are already verified or not.</li>
<li>Select <strong>Reverify all failed</strong>: this resets all records for which verification failed, but sync is successful.</li>
</ol>
<h5>Reverify one component on all sites</h5>
<p>If the <strong>primary</strong> site's checksums are in question, then you need to make the <strong>primary</strong> site recalculate checksums.
A "full re-verification" is then achieved, because after each checksum is recalculated on a <strong>primary</strong> site, events
are generated which propagate to all <strong>secondary</strong> sites, causing them to recalculate their checksums and compare values.
Any mismatch marks the registry as <code>sync failed</code>, which causes sync retries to be scheduled.</p>
<p>You can recalculate the primary site's checksum from the UI:</p>
<ol>
<li>In the upper-right corner, select <strong>Admin</strong>.</li>
<li>Select <strong>Monitoring</strong> &gt; <strong>Data management</strong>.</li>
<li>Select the desired component in the dropdown list.</li>
<li>Select <strong>Checksum all</strong>.</li>
</ol>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p><strong>Resync all</strong>, <strong>Reverify all</strong> and <strong>Checksum all</strong> trigger an update of all resources, regardless of whether they are already synced or verified.
It should not be executed when there are thousands of an object type in the instance (for example, CI Job Artifacts).</p>
<p>{{&lt; /alert &gt;}}</p>
<h4>From the Rails console</h4>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>Commands that change data can cause damage if not run correctly or under the right conditions.
Always run commands in a test environment first and have a backup instance ready to restore.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>The following sections describe how to use internal application commands in the
<a href="../../../operations/rails_console.md#starting-a-rails-console-session">Rails console</a> to cause bulk
replication or verification.</p>
<h5>Sync all resources of one component that failed to sync</h5>
<p>The following script:</p>
<ul>
<li>Loops over all failed repositories.</li>
<li>Displays the Geo sync and verification metadata, including the reasons for the last failure.</li>
<li>Attempts to resync the repository.</li>
<li>Reports back if a failure occurs, and why.</li>
<li>Might take some time to complete. Each repository check must complete
  before reporting back the result. If your session times out, take measures
  to allow the process to continue running such as starting a <code>screen</code> session,
  or running it using <a href="../../../operations/rails_console.md#using-the-rails-runner">Rails runner</a>
  and <code>nohup</code>.</li>
</ul>
<p>Run this script <strong>on the secondary Geo site</strong>.</p>
<pre><code class="language-ruby">Geo::ProjectRepositoryRegistry.failed.find_each do |registry|
   begin
     puts &quot;ID: #{registry.id}, Project ID: #{registry.project_id}, Last Sync Failure: '#{registry.last_sync_failure}'&quot;
     registry.replicator.sync
     puts &quot;Sync initiated for registry ID: #{registry.id}&quot;
   rescue =&gt; e
     puts &quot;ID: #{registry.id}, Project ID: #{registry.project_id}, Failed: '#{e}'&quot;, e.backtrace.join(&quot;\n&quot;)
   end
end; nil
</code></pre>
<h5>Reverify all resources that failed to checksum on the primary site</h5>
<p>The system automatically reverifies all resources that failed to checksum on the primary site, but
it uses a progressive backoff scheme to avoid an excessive volume of failures.</p>
<p>Optionally, for example if you've completed an attempted intervention, you can manually trigger
reverification sooner:</p>
<ol>
<li>SSH into a GitLab Rails node in the <strong>primary</strong> site.</li>
<li>Open the <a href="../../../operations/rails_console.md#starting-a-rails-console-session">Rails console</a>.</li>
<li>Replacing <code>Upload</code> with any of the <a href="#geo-data-type-model-classes">Geo data type Model classes</a>,
   mark all resources as <code>pending verification</code>:</li>
</ol>
<p><code>ruby
   Upload.verification_state_table_class.where(verification_state: 3).each_batch do |relation|
     relation.update_all(verification_state: 0)
   end</code></p>
<h2>Errors</h2>
<h3>Message: <code>The file is missing on the Geo primary site</code></h3>
<p>The sync failure <code>The file is missing on the Geo primary site</code> is common when
setting up a secondary Geo site for the first time, which is caused by data
inconsistencies on the primary site.</p>
<p>Data inconsistencies and missing files can occur due to system or human errors
when operating GitLab. For example, an instance administrator manually deletes
several artifacts on the local file system. Such changes are not properly
propagated to the database and result in inconsistencies. These inconsistencies
remain and can cause frictions. Geo secondaries might continue to try
replicating those files as they are still referenced in the database but no
longer exist.</p>
<p>{{&lt; alert type="note" &gt;}}</p>
<p>In case of a recent migration from local to object storage, see the dedicated
<a href="../../../object_storage.md#inconsistencies-after-migrating-to-object-storage">object storage troubleshooting section</a>.</p>
<p>{{&lt; /alert &gt;}}</p>
<h4>Identify inconsistencies</h4>
<p>When missing files or inconsistencies are present, you can encounter entries in <code>geo.log</code> such as the following. Take note of the field <code>"primary_missing_file" : true</code>:</p>
<pre><code class="language-json">{
   &quot;bytes_downloaded&quot; : 0,
   &quot;class&quot; : &quot;Geo::BlobDownloadService&quot;,
   &quot;correlation_id&quot; : &quot;01JT69C1ECRBEMZHA60E5SAX8E&quot;,
   &quot;download_success&quot; : false,
   &quot;download_time_s&quot; : 0.196,
   &quot;gitlab_host&quot; : &quot;gitlab.example.com&quot;,
   &quot;mark_as_synced&quot; : false,
   &quot;message&quot; : &quot;Blob download&quot;,
   &quot;model_record_id&quot; : 55,
   &quot;primary_missing_file&quot; : true,
   &quot;reason&quot; : &quot;Not Found&quot;,
   &quot;replicable_name&quot; : &quot;upload&quot;,
   &quot;severity&quot; : &quot;WARN&quot;,
   &quot;status_code&quot; : 404,
   &quot;time&quot; : &quot;2025-05-01T16:02:44.836Z&quot;,
   &quot;url&quot; : &quot;http://gitlab.example.com/api/v4/geo/retrieve/upload/55&quot;
}
</code></pre>
<p>The same errors are also reflected in the UI under <strong>Admin</strong> &gt; <strong>Geo</strong> &gt; <strong>Sites</strong> when reviewing the synchronization status of specific replicables. In this scenario, a specific upload is missing:</p>
<p><img alt="The Geo Uploads replicable dashboard displaying all failed errors." src="img/geo_uploads_file_missing_v17_11.png" /></p>
<p><img alt="The Geo Uploads replicable dashboard displaying missing file error." src="img/geo_uploads_file_missing_details_v17_11.png" /></p>
<h4>Clean up inconsistencies</h4>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>Ensure you have a recent and working backup at hand before issuing any deletion commands.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>To remove those errors, first identify which particular resources are affected. Then, run the appropriate <code>destroy</code> commands to ensure the deletion is propagated across all Geo sites and their databases. Based on the previous scenario, an <strong>upload</strong> is causing those errors which is used as an example below.</p>
<ol>
<li>Map the identified inconsistencies to their respective <a href="#geo-data-type-model-classes">Geo Model class</a> name. The class name is needed in the following steps. In this scenario, for uploads it corresponds to <code>Upload</code>.</li>
<li>Start a <a href="../../../operations/rails_console.md#starting-a-rails-console-session">Rails console</a> on the <strong>Geo primary site</strong>.</li>
<li>Query all resources where verification failed due to missing files based on the <em>Geo Model class</em> of the previous step. Adjust or remove the <code>limit(20)</code> to display more results. Observe how the listed resources should match the failed ones shown in the UI:</li>
</ol>
<p>```ruby
   Upload.verification_failed.where("verification_failure like '%File is not checksummable%'").limit(20)</p>
<p>=&gt; #<Upload:0x00007b362bb6c4e8
    id: 55,
    size: 13346,
    path: "503d99159e2aa8a3ac23602058cfdf58/openbao.png",
    checksum: "db29d233de49b25d2085dcd8610bac787070e721baa8dcedba528a292b6e816b",
    model_id: 1,
    model_type: "Project",
    uploader: "FileUploader",
    created_at: Thu, 01 May 2025 15:54:10.549178000 UTC +00:00,
    store: 1,
    mount_point: nil,
    secret: "[FILTERED]",
    version: 2,
    uploaded_by_user_id: 1,
    organization_id: nil,
    namespace_id: nil,
    project_id: 1,
    verification_checksum: nil>
   ```</p>
<ol>
<li>Optionally, use the <code>id</code> of the affected resources to determine if they are still needed:</li>
</ol>
<p>```ruby
   Upload.find(55)</p>
<p>=&gt; #<Upload:0x00007b362bb6c4e8
    id: 55,
    size: 13346,
    path: "503d99159e2aa8a3ac23602058cfdf58/openbao.png",
    checksum: "db29d233de49b25d2085dcd8610bac787070e721baa8dcedba528a292b6e816b",
    model_id: 1,
    model_type: "Project",
    uploader: "FileUploader",
    created_at: Thu, 01 May 2025 15:54:10.549178000 UTC +00:00,
    store: 1,
    mount_point: nil,
    secret: "[FILTERED]",
    version: 2,
    uploaded_by_user_id: 1,
    organization_id: nil,
    namespace_id: nil,
    project_id: 1,
    verification_checksum: nil>
   ```</p>
<ul>
<li>
<p>If you determine that the affected resources need to be recovered, then you can explore the following options (non-exhaustive) to recover them:</p>
<ul>
<li>Check if the secondary site has the object and manually copy them to the primary.</li>
<li>Look through old backups and manually copy the object back into the primary site.</li>
<li>Spot check some to try to determine that it's probably fine to destroy the records, for example, if they are all very old artifacts, then maybe they are not critical data.</li>
</ul>
</li>
<li>
<p>Use the <code>id</code> of the identified resources to properly delete them individually or in bulk by using <code>destroy</code>. Ensure to use the appropriate <em>Geo Model class</em> name.</p>
</li>
<li>
<p>Delete individual resources:</p>
<p><code>ruby
 Upload.find(55).destroy</code></p>
</li>
<li>
<p>Delete all affected resources:</p>
<p>```ruby
 def destroy_uploads_not_checksummable
   uploads = Upload.verification_failed.where("verification_failure like '%File is not checksummable%'");1
   puts "Found #{uploads.count} resources that failed verification with 'File is not checksummable'."
   puts "Enter 'y' to continue: "
   prompt = STDIN.gets.chomp
   if prompt != 'y'
     puts "Exiting without action..."
     return
   end</p>
<p>puts "Destroying all..."
   uploads.destroy_all
 end</p>
<p>destroy_uploads_not_checksummable
 ```</p>
</li>
</ul>
<p>Repeat the steps for all affected resources and Geo data types.</p>
<h3>Message: <code>"Error during verification","error":"File is not checksummable"</code></h3>
<p>The error <code>"Error during verification","error":"File is not checksummable"</code> is caused by inconsistencies on the primary site. Follow the instructions provided in <a href="#message-the-file-is-missing-on-the-geo-primary-site">The file is missing on the Geo primary site</a>.</p>
<h3>Failed verification of Uploads on the primary Geo site</h3>
<p>If verification of some uploads is failing on the primary Geo site with <code>verification_checksum = nil</code> and with <code>verification_failure</code> containing <code>Error during verification: undefined method `underscore' for NilClass:Class</code> or <code>The model which owns this Upload is missing.</code>, this is due to orphaned Uploads. The parent record owning the Upload (the upload's "model") has somehow been deleted, but the Upload record still exists. This is usually due to a bug in the application, introduced by implementing bulk delete of the "model" while forgetting to bulk delete its associated Upload records. These verification failures are therefore not failures to verify, rather, the errors are a result of bad data in Postgres.</p>
<p>You can find these errors in the <code>geo.log</code> file on the primary Geo site.</p>
<p>To confirm that model records are missing, you can run a Rake task on the primary Geo site:</p>
<pre><code class="language-shell">sudo gitlab-rake gitlab:uploads:check
</code></pre>
<p>You can delete these Upload records on the primary Geo site to get rid of these failures by running the following script from the <a href="../../../operations/rails_console.html">Rails console</a>:</p>
<pre><code class="language-ruby">def delete_orphaned_uploads(dry_run: true)
  if dry_run
    p &quot;This is a dry run. Upload rows will only be printed.&quot;
  else
    p &quot;This is NOT A DRY RUN! Upload rows will be deleted from the DB!&quot;
  end

  subquery = Geo::UploadState.where(&quot;(verification_failure LIKE 'Error during verification: The model which owns this Upload is missing.%' OR verification_failure = 'Error during verification: undefined method `underscore'' for NilClass:Class') AND verification_checksum IS NULL&quot;)
  uploads = Upload.where(upload_state: subquery)
  p &quot;Found #{uploads.count} uploads with a model that does not exist&quot;

  uploads_deleted = 0
  begin
    uploads.each do |upload|

      if dry_run
        p upload
      else
        uploads_deleted=uploads_deleted + 1
        p upload.destroy!
      end
    rescue =&gt; e
      puts &quot;checking upload #{upload.id} failed with #{e.message}&quot;
    end
  end

  p &quot;#{uploads_deleted} remote objects were destroyed.&quot; unless dry_run
end
</code></pre>
<p>The previous script defines a method named <code>delete_orphaned_uploads</code> which you can call like this to do a dry run:</p>
<pre><code class="language-ruby">delete_orphaned_uploads(dry_run: true)
</code></pre>
<p>And to actually delete the orphaned upload rows:</p>
<pre><code class="language-ruby">delete_orphaned_uploads(dry_run: false)
</code></pre>
<h3>Error: <code>Error syncing repository: 13:fatal: could not read Username</code></h3>
<p>The <code>last_sync_failure</code> error
<code>Error syncing repository: 13:fatal: could not read Username for 'https://gitlab.example.com': terminal prompts disabled</code>
indicates that JWT authentication is failing during a Geo clone or fetch request.</p>
<p>First, check that system clocks are synced. Run the <a href="common.md#health-check-rake-task">Health check Rake task</a>, or
manually check that <code>date</code>, on all Sidekiq nodes on the secondary site and all Puma nodes on the primary site, are the
same.</p>
<p>If system clocks are synced, then the JWT token may be expiring while Git fetch is performing calculations between its
two separate HTTP requests. See <a href="https://gitlab.com/gitlab-org/gitlab/-/issues/464101">issue 464101</a>, which existed in
all GitLab versions until it was fixed in GitLab 17.1.0, 17.0.5, and 16.11.7.</p>
<p>To validate if you are experiencing this issue:</p>
<ol>
<li>Monkey patch the code in a <a href="../../../operations/rails_console.md#starting-a-rails-console-session">Rails console</a> to increase the validity period of the token from 1 minute to 10 minutes. Run
   this in Rails console on the secondary site:</li>
</ol>
<p>```ruby
   module Gitlab; module Geo; class BaseRequest
     private
     def geo_auth_token(message)
       signed_data = Gitlab::Geo::SignedData.new(geo_node: requesting_node, validity_period: 10.minutes).sign_and_encode_data(message)</p>
<pre><code>   "#{GITLAB_GEO_AUTH_TOKEN_TYPE} #{signed_data}"
 end
</code></pre>
<p>end;end;end
   ```</p>
<ol>
<li>In the same Rails console, resync an affected project:</li>
</ol>
<p><code>ruby
   Project.find_by_full_path('&lt;mygroup/mysubgroup/myproject&gt;').replicator.resync</code></p>
<ol>
<li>Look at the sync state:</li>
</ol>
<p><code>ruby
   Project.find_by_full_path('&lt;mygroup/mysubgroup/myproject&gt;').replicator.registry</code></p>
<ol>
<li>If <code>last_sync_failure</code> no longer includes the error <code>fatal: could not read Username</code>, then you are
   affected by this issue. The state should now be <code>2</code>, which means that it's synced. If so, then you should upgrade to
   a GitLab version with the fix. You may also wish to upvote or comment on
   <a href="https://gitlab.com/gitlab-org/gitlab/-/issues/466681">issue 466681</a> which would have reduced the severity of this
   issue.</li>
</ol>
<p>To workaround the issue, you must hot-patch all Sidekiq nodes in the secondary site to extend the JWT expiration time:</p>
<ol>
<li>Edit <code>/opt/gitlab/embedded/service/gitlab-rails/ee/lib/gitlab/geo/signed_data.rb</code>.</li>
<li>Find <code>Gitlab::Geo::SignedData.new(geo_node: requesting_node)</code> and add <code>, validity_period: 10.minutes</code> to it:</li>
</ol>
<p><code>diff
   - Gitlab::Geo::SignedData.new(geo_node: requesting_node)
   + Gitlab::Geo::SignedData.new(geo_node: requesting_node, validity_period: 10.minutes)</code></p>
<ol>
<li>Restart Sidekiq:</li>
</ol>
<p><code>shell
   sudo gitlab-ctl restart sidekiq</code></p>
<ol>
<li>Unless you upgrade to a version containing the fix, you would have to repeat this workaround after every GitLab upgrade.</li>
</ol>
<h3>Error: <code>Error syncing repository: 13:creating repository: cloning repository: exit status 128</code></h3>
<p>You might see this error for projects that do not sync successfully.</p>
<p>Exit code 128 during repository creation means Git encountered a fatal error while cloning. This could be due to repository corruption, network issues, authentication problems, resource limits or because the project does not have an associated Git repository. More details about the specific cause for such failures can be found in the Gitaly logs.</p>
<p>When unsure where to start, run an integrity check on the source repository on the Primary site by <a href="../../../../administration/repository_checks.md#run-a-check-using-the-command-line">executing the <code>git fsck</code> command manually on the command line</a>.</p>
<h3>Error: <code>gitmodulesUrl: disallowed submodule url</code></h3>
<p>Some project repositories consistently fail to sync with the error
<code>Error syncing repository: 13:creating repository: cloning repository: exit status 128</code>. However,
for some repositories, the specific error message in the Gitaly logs is different: <code>gitmodulesUrl: disallowed submodule url</code>.
This failure happens when repositories contain invalid submodule URLs in their <code>.gitmodules</code> files.</p>
<p><strong>Root Cause:</strong> This issue is caused by <strong>historical commits</strong> in the Git repository that contain <code>.gitmodules</code> files with malformed URLs. The problem occurs during Git's consistency checks (<code>git fsck</code>) that run when Geo attempts to clone the repository from primary to secondary.</p>
<p>The problem is in the repository's commit history. Submodule URLs in <code>.gitmodules</code> files contain
invalid formats, using <code>:</code> instead of <code>/</code> in the path:</p>
<ul>
<li>Invalid: <code>https://example.gitlab.com:group/project.git</code></li>
<li>Valid: <code>https://example.gitlab.com/group/project.git</code></li>
</ul>
<p><strong>Why this breaks Geo synchronization:</strong></p>
<ol>
<li><strong>Git's strict validation</strong>: Starting with GitLab 17.0 and newer Git versions, Git performs stricter <code>fsck</code> checks during clone operations</li>
<li><strong>Historical data persistence</strong>: Even if the current <code>.gitmodules</code> file is correct, Git stores all historical versions as "blobs" in the repository</li>
<li><strong>Clone-time failure</strong>: When Geo tries to clone the repository, Git's <code>fsck</code> examines <strong>all objects</strong> (including historical ones) and fails when it finds malformed URLs</li>
<li><strong>Complete sync failure</strong>: The entire clone operation fails, preventing the repository from reaching the secondary site</li>
</ol>
<p><strong>Important:</strong> Editing the current <code>.gitmodules</code> file does <strong>not</strong> resolve this issue because the problematic data exists in the repository's Git history, not just in the current version of the file.</p>
<p>This issue is known in GitLab 17.0 and later, and is a result of more strict repository consistency
checks. This new behavior results from a change in Git itself, where this check was added. It is not
specific to GitLab Geo or Gitaly. For more information, see
<a href="https://gitlab.com/gitlab-org/gitlab/-/issues/468560">issue 468560</a>.</p>
<h4>Workaround</h4>
<ol>
<li><strong>Backup projects</strong></li>
</ol>
<p>Before proceeding, ensure they back up the projects beforehand, using the <a href="../../../../user/project/settings/import_export.html">project export option</a>.</p>
<ol>
<li><strong>Identify problematic blob IDs</strong></li>
</ol>
<p>For each affected project, identify the problematic blob IDs using one of these methods:</p>
<ul>
<li>
<p>Use <code>git fsck</code>: Clone the repository, then run <code>git fsck</code> to confirm the issue:</p>
<p><code>shell
 git clone https://example.gitlab.com/group/project.git
 cd project
 git fsck</code></p>
<p>The output shows the problematic blob:</p>
<p><code>plaintext
 Checking object directories: 100% (256/256), done.
 error in blob &lt;SHA&gt;: gitmodulesUrl: disallowed submodule url: https://example.gitlab.com:group/project.git
 Checking objects: 100% (12/12), done.</code></p>
</li>
<li>
<p>Check the Gitaly logs. Look for error messages containing <code>gitmodulesUrl</code>
     to find the specific blob SHA.</p>
</li>
<li>
<p><strong>Remove blobs</strong></p>
</li>
</ul>
<p>For each affected project, <a href="../../../../user/project/repository/repository_size.md#remove-blobs">remove the problematic blob IDs</a> identified in the previous step.</p>
<p><strong>Important limitation:</strong> If any of these repositories are part of a fork network, the blob removal method may not work (blobs contained in object pools cannot be removed this way).</p>
<ol>
<li><strong>Fix .gitmodules invalid URLs if required</strong></li>
</ol>
<p>Check the state of <code>.gitmodules</code> files in each affected repository</p>
<p>If the <code>.gitmodules</code> still contains invalid URLs like <code>https://example.gitlab.com:foo/bar.git</code> instead of <code>https://example.gitlab.com/foo/bar.git</code>, the customer needs to:</p>
<ul>
<li>Fix the URLs in the <code>.gitmodules</code> file</li>
<li>Push a commit with valid URLs</li>
</ul>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>After the fix, all developers working on the affected projects must remove their current local copies
and clone fresh repositories. Otherwise, they might reintroduce the offending blobs when pushing changes.</p>
<p>{{&lt; /alert &gt;}}</p>
<h3>Error: <code>fetch remote: signal: terminated: context deadline exceeded</code> at exactly 3 hours</h3>
<p>If Git fetch fails at exactly three hours while syncing a Git repository:</p>
<ol>
<li>Edit <code>/etc/gitlab/gitlab.rb</code> to increase the Git timeout from the default of 10800 seconds:</li>
</ol>
<p><code>ruby
   # Git timeout in seconds
   gitlab_rails['gitlab_shell_git_timeout'] = 21600</code></p>
<ol>
<li>Reconfigure GitLab:</li>
</ol>
<p><code>shell
   sudo gitlab-ctl reconfigure</code></p>
<h3>Error <code>Failed to open TCP connection to localhost:5000</code> on secondary when configuring registry replication</h3>
<p>You may face the following error when configuring container registry replication on the secondary site:</p>
<pre><code class="language-plaintext">Failed to open TCP connection to localhost:5000 (Connection refused - connect(2) for \&quot;localhost\&quot; port 5000)&quot;
</code></pre>
<p>It happens if the container registry is not enabled on the secondary site. To fix it, check that the container registry
is <a href="../../../packages/container_registry.md#enable-the-container-registry">enabled on the secondary site</a>. If the <a href="https://docs.gitlab.com/omnibus/settings/ssl/#configure-https-manually">Let's Encrypt integration is disabled</a>, container registry is disabled as well, and you must <a href="../../../packages/container_registry.md#configure-container-registry-under-its-own-domain">configure it manually</a>.</p>
<h3>Error: <code>Verification timed out after 28800</code></h3>
<p><strong>Possible Root Cause:</strong> Duplicate registry records causing verification conflicts across various registry types.</p>
<p><strong>Diagnosis:</strong></p>
<p>Check for duplicate registries across different types on the secondary site:</p>
<pre><code class="language-ruby"># Check for duplicate upload registries
upload_ids = Geo::UploadRegistry.group(:file_id).having('COUNT(*) &gt; 1').pluck(:file_id)
puts &quot;Duplicate upload IDs count: #{upload_ids.size}&quot;
puts 'Duplicate Upload IDs:', upload_ids

# Check for duplicate job artifact registries
artifact_ids = Geo::JobArtifactRegistry.group(:artifact_id).having('COUNT(*) &gt; 1').pluck(:artifact_id)
puts &quot;Duplicate artifact IDs count: #{artifact_ids.size}&quot;
puts 'Duplicate Artifact IDs:', artifact_ids

# Check for duplicate package file registries
package_file_ids = Geo::PackageFileRegistry.group(:package_file_id).having('COUNT(*) &gt; 1').pluck(:package_file_id)
puts &quot;Duplicate package file IDs count: #{package_file_ids.size}&quot;
puts 'Duplicate Package File IDs:', package_file_ids

# Check for duplicate LFS object registries
lfs_object_ids = Geo::LfsObjectRegistry.group(:lfs_object_id).having('COUNT(*) &gt; 1').pluck(:lfs_object_id)
puts &quot;Duplicate LFS object IDs count: #{lfs_object_ids.size}&quot;
puts 'Duplicate LFS Object IDs:', lfs_object_ids

# Check for duplicate pages deployment registries
pages_deployment_ids = Geo::PagesDeploymentRegistry.group(:pages_deployment_id).having('COUNT(*) &gt; 1').pluck(:pages_deployment_id)
puts &quot;Duplicate pages deployment IDs count: #{pages_deployment_ids.size}&quot;
puts 'Duplicate Pages Deployment IDs:', pages_deployment_ids

# Check for duplicate terraform state version registries
terraform_state_ids = Geo::TerraformStateVersionRegistry.group(:terraform_state_version_id).having('COUNT(*) &gt; 1').pluck(:terraform_state_version_id)
puts &quot;Duplicate terraform state version IDs count: #{terraform_state_ids.size}&quot;
puts 'Duplicate Terraform State Version IDs:', terraform_state_ids
</code></pre>
<p><strong>Resolution:</strong></p>
<ol>
<li>Remove duplicate registry entries for each affected type:</li>
</ol>
<p>```ruby
   # Remove duplicate upload registries
   upload_ids = Geo::UploadRegistry.group(:file_id).having('COUNT(*) &gt; 1').pluck(:file_id)
   if upload_ids.any?
     Geo::UploadRegistry.where(file_id: upload_ids).delete_all
     puts "Removed #{upload_ids.size} duplicate upload registry entries"
   end</p>
<p># Remove duplicate job artifact registries
   artifact_ids = Geo::JobArtifactRegistry.group(:artifact_id).having('COUNT(*) &gt; 1').pluck(:artifact_id)
   if artifact_ids.any?
     Geo::JobArtifactRegistry.where(artifact_id: artifact_ids).delete_all
     puts "Removed #{artifact_ids.size} duplicate job artifact registry entries"
   end</p>
<p># Remove duplicate package file registries
   package_file_ids = Geo::PackageFileRegistry.group(:package_file_id).having('COUNT(*) &gt; 1').pluck(:package_file_id)
   if package_file_ids.any?
     Geo::PackageFileRegistry.where(package_file_id: package_file_ids).delete_all
     puts "Removed #{package_file_ids.size} duplicate package file registry entries"
   end</p>
<p># Remove duplicate LFS object registries
   lfs_object_ids = Geo::LfsObjectRegistry.group(:lfs_object_id).having('COUNT(*) &gt; 1').pluck(:lfs_object_id)
   if lfs_object_ids.any?
     Geo::LfsObjectRegistry.where(lfs_object_id: lfs_object_ids).delete_all
     puts "Removed #{lfs_object_ids.size} duplicate LFS object registry entries"
   end</p>
<p># Remove duplicate pages deployment registries
   pages_deployment_ids = Geo::PagesDeploymentRegistry.group(:pages_deployment_id).having('COUNT(*) &gt; 1').pluck(:pages_deployment_id)
   if pages_deployment_ids.any?
     Geo::PagesDeploymentRegistry.where(pages_deployment_id: pages_deployment_ids).delete_all
     puts "Removed #{pages_deployment_ids.size} duplicate pages deployment registry entries"
   end</p>
<p># Remove duplicate terraform state version registries
   terraform_state_ids = Geo::TerraformStateVersionRegistry.group(:terraform_state_version_id).having('COUNT(*) &gt; 1').pluck(:terraform_state_version_id)
   if terraform_state_ids.any?
     Geo::TerraformStateVersionRegistry.where(terraform_state_version_id: terraform_state_ids).delete_all
     puts "Removed #{terraform_state_ids.size} duplicate terraform state version registry entries"
   end
   ```</p>
<ol>
<li>Verify cleanup across all registry types:</li>
</ol>
<p>```ruby
   # Verify no remaining duplicates
   upload_duplicates = Geo::UploadRegistry.group(:file_id).having('COUNT(<em>) &gt; 1').count
   artifact_duplicates = Geo::JobArtifactRegistry.group(:artifact_id).having('COUNT(</em>) &gt; 1').count
   package_duplicates = Geo::PackageFileRegistry.group(:package_file_id).having('COUNT(<em>) &gt; 1').count
   lfs_duplicates = Geo::LfsObjectRegistry.group(:lfs_object_id).having('COUNT(</em>) &gt; 1').count
   pages_duplicates = Geo::PagesDeploymentRegistry.group(:pages_deployment_id).having('COUNT(<em>) &gt; 1').count
   terraform_duplicates = Geo::TerraformStateVersionRegistry.group(:terraform_state_version_id).having('COUNT(</em>) &gt; 1').count</p>
<p>puts "Remaining duplicates:"
   puts "  Uploads: #{upload_duplicates.size}"
   puts "  Job Artifacts: #{artifact_duplicates.size}"
   puts "  Package Files: #{package_duplicates.size}"
   puts "  LFS Objects: #{lfs_duplicates.size}"
   puts "  Pages Deployments: #{pages_duplicates.size}"
   puts "  Terraform State Versions: #{terraform_duplicates.size}"
   ```</p>
<h3>Error: <code>Checksum does not match the primary checksum</code></h3>
<p><strong>Possible Root Cause:</strong> Repository or Container Registry verification interval changes causing checksum inconsistencies.</p>
<p><strong>Diagnosis:</strong></p>
<p>Check failed repositories or container registries in secondary:</p>
<pre><code class="language-ruby">failed_repos = Geo::ProjectRepositoryRegistry.failed.limit(100)
failed_repos.each do |repo|
  puts &quot;Project ID: #{repo.project_id}&quot;
  puts &quot;Primary checksum: #{repo.verification_checksum_mismatched}&quot;
  puts &quot;Secondary checksum: #{repo.verification_checksum}&quot;
  puts &quot;Error: #{repo.last_sync_failure}&quot;
  puts &quot;---&quot;
end
</code></pre>
<pre><code class="language-ruby">failed_container_repos = Geo::ContainerRepositoryRegistry.failed.limit(100)
failed_container_repos.each do |repo|
  puts &quot;Container Repo Id: #{repo.model_record_id}&quot;
  puts &quot;Primary checksum: #{repo.verification_checksum_mismatched}&quot;
  puts &quot;Secondary checksum: #{repo.verification_checksum}&quot;
  puts &quot;Error: #{repo.last_sync_failure}&quot;
  puts &quot;---&quot;
end
</code></pre>
<p><strong>Resolution:</strong></p>
<p>Force re-verification on primary for specific projects or container registries:</p>
<pre><code class="language-ruby">project_ids = [1, 2, 3] # Replace with actual failing project IDs

project_ids.each do |project_id|
  project = Project.find(project_id)
  puts &quot;Reverifying project: #{project.full_path}&quot;

  project_state = project.project_state
  project_state.update!(verification_state: 0)

  puts &quot;Project #{project_id} marked for reverification&quot;
end
</code></pre>
<pre><code class="language-ruby">container_repo_ids = [1, 2, 3]

container_repo_ids.each do |repo_id|
  container_repo = ContainerRepository.find(repo_id)
  puts &quot;Reverifying container repository: #{container_repo.path}&quot;

  state = container_repo.container_repository_state
  state.update!(verification_state: 0)

  puts &quot;Container Repo #{repo_id} marked for reverification&quot;
end
</code></pre>
<h3>Object type-specific troubleshooting for <code>Error during verification: File is not checksummable</code></h3>
<p>Different Geo data types have unique characteristics and common failure patterns. This section provides targeted troubleshooting for specific object types.</p>
<h4>Uploads</h4>
<p><strong>Diagnosis:</strong></p>
<p>Identify uploads with missing files:</p>
<pre><code class="language-ruby">checksummable_failures = Upload.verification_failed
                                .where(&quot;verification_failure LIKE '%File is not checksummable%'&quot;)

puts &quot;Found #{checksummable_failures.count} uploads with missing files&quot;

# Adjust 'limit' to count
checksummable_failures.limit(5).each_with_index do |record, index|
  puts &quot;Record #{index + 1}:&quot;
  puts &quot;  ID: #{record.id}&quot;
  puts &quot;  Path: #{record.path}&quot;
  puts &quot;  Model: #{record.model_type} (ID: #{record.model_id})&quot;
  puts &quot;  Created: #{record.created_at}&quot;
  puts &quot;---&quot;
end
</code></pre>
<p><strong>Resolution:</strong></p>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>Ensure you have a recent and working backup before deleting any upload records. Coordinate with your team to confirm these uploads are safe to remove.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>Remove problematic uploads after confirmation:</p>
<pre><code class="language-ruby"># Remove individual upload
Upload.find(55).destroy

# Or remove all uploads with missing files (use with extreme caution)
Upload.verification_failed.where(&quot;verification_failure LIKE '%File is not checksummable%'&quot;).destroy_all
</code></pre>
<h4>Pages deployments</h4>
<p><strong>Diagnosis:</strong></p>
<p>Inspect problematic pages deployments:</p>
<pre><code class="language-ruby">checksummable_failures = PagesDeployment.verification_failed
                                        .where(&quot;verification_failure LIKE '%File is not checksummable%'&quot;)

checksummable_failures.each_with_index do |record, index|
  puts &quot;Record #{index + 1}:&quot;
  puts &quot;  ID: #{record.id}&quot;
  puts &quot;  Project: #{record.project.full_path}&quot;
  puts &quot;  Created: #{record.created_at}&quot;
  puts &quot;  File exists: #{record.file.exists?}&quot;
  puts &quot;---&quot;
end
</code></pre>
<p><strong>Resolution:</strong></p>
<p>After confirming with your team that the deployments are safe to remove:</p>
<pre><code class="language-ruby">failed_ids = [21875, 21907, 21992] # Replace with actual IDs
PagesDeployment.where(id: failed_ids).destroy_all
puts &quot;Removed #{failed_ids.size} problematic pages deployments&quot;
</code></pre>
<h4>LFS objects</h4>
<p><strong>Diagnosis:</strong></p>
<p>Inspect problematic LFS objects:</p>
<pre><code class="language-ruby">checksummable_failures = LfsObject.verification_failed
                                  .where(&quot;verification_failure LIKE '%File is not checksummable%'&quot;)

checksummable_failures.each_with_index do |record, index|
  puts &quot;Record #{index + 1}:&quot;
  puts &quot;  OID: #{record.oid}&quot;
  puts &quot;  Size: #{record.size} bytes&quot;
  puts &quot;  File Store: #{record.file_store}&quot;
  puts &quot;  Created: #{record.created_at}&quot;

  # Show associated projects
  associations = record.lfs_objects_projects.includes(:project)
  puts &quot;  Associated projects (#{associations.count}):&quot;
  associations.each do |assoc|
    project = assoc.project
    if project
      puts &quot;    - #{project.full_path}&quot;
    else
      puts &quot;    - Project ID: #{assoc.project_id} (not found)&quot;
    end
  end
  puts &quot;---&quot;
end
</code></pre>
<p><strong>Resolution:</strong></p>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>Removing LFS objects affects all projects that reference them. Ensure you have backups and coordinate with project maintainers before deletion.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>Remove LFS objects with missing files:</p>
<pre><code class="language-ruby">def destroy_lfs_not_checksummable(dry_run: true)
  lfs_objects = LfsObject.verification_failed.where(&quot;verification_failure like '%File is not checksummable%'&quot;)
  puts &quot;Found #{lfs_objects.count} LFS objects that failed verification with 'File is not checksummable'.&quot;

  if dry_run
    puts &quot;DRY RUN - No changes made&quot;
    lfs_objects.each { |obj| puts &quot;Would remove: OID #{obj.oid}, Size: #{obj.size}&quot; }
    return
  end

  puts &quot;Enter 'y' to continue with deletion: &quot;
  prompt = STDIN.gets.chomp
  if prompt != 'y'
    puts &quot;Exiting without action...&quot;
    return
  end

  puts &quot;Destroying all...&quot;
  lfs_objects.each do |lfs_object|
    lfs_object.lfs_objects_projects.destroy_all
    lfs_object.destroy!
  end
  puts &quot;Done!&quot;
end

# Run in dry run mode first
destroy_lfs_not_checksummable(dry_run: true)
</code></pre>
<h4>Job artifacts</h4>
<p><strong>Diagnosis:</strong></p>
<p>Check for artifacts with missing files:</p>
<pre><code class="language-ruby">failed_artifacts = Ci::JobArtifact.verification_failed.where(&quot;verification_failure LIKE '%File is not checksummable%'&quot;)

failed_artifacts.each do |registry|
  artifact = Ci::JobArtifact.find_by(id: registry.id)
  if artifact
    puts &quot;Artifact ID: #{artifact.id}&quot;
    puts &quot;Job ID: #{artifact.job_id}&quot;
    puts &quot;Project ID: #{artifact.project_id}&quot;
    puts &quot;File exists: #{artifact.file.exists?}&quot;
    puts &quot;File path: #{artifact.file.path}&quot;
  else
    puts &quot;Artifact ID #{artifact.id} not found in database&quot;
  end
  puts &quot;---&quot;
end
</code></pre>
<p><strong>Resolution:</strong></p>
<p>Clean up artifacts with missing files:</p>
<pre><code class="language-ruby">def cleanup_missing_artifacts(dry_run: true)
  missing_file_artifacts = []

  Ci::JobArtifact.find_each do |artifact|
    unless artifact.file.exists?
      missing_file_artifacts &lt;&lt; artifact.id
      puts &quot;Missing file for artifact #{artifact.id}&quot; if dry_run
    end
  end

  puts &quot;Found #{missing_file_artifacts.size} artifacts with missing files&quot;

  unless dry_run
    Ci::JobArtifact.where(id: missing_file_artifacts).destroy_all
    puts &quot;Removed #{missing_file_artifacts.size} artifacts with missing files&quot;
  end
end

# Run in dry run mode first
cleanup_missing_artifacts(dry_run: true)
</code></pre>
<h4>Pipeline artifacts</h4>
<p><strong>Diagnosis:</strong></p>
<p>Check for artifacts with missing files:</p>
<pre><code class="language-ruby">failed_pipeline_artifacts = Ci::PipelineArtifact.verification_failed.where(&quot;verification_failure LIKE '%checksummable%'&quot;)

failed_pipeline_artifacts.each do |registry|
  artifact = Ci::PipelineArtifact.find_by(id: registry.id)
  if artifact
    puts &quot;Artifact ID: #{artifact.id}&quot;
    puts &quot;Pipeline ID: #{artifact.pipeline_id}&quot;
    puts &quot;Project ID: #{artifact.project_id}&quot;
    puts &quot;File exists: #{artifact.file.exists?}&quot;
    puts &quot;File path: #{artifact.file.path}&quot;
  else
    puts &quot;Artifact ID #{artifact.id} not found in database&quot;
  end
  puts &quot;---&quot;
end
</code></pre>
<p><strong>Resolution:</strong></p>
<p>Remove pipeline artifacts with missing files:</p>
<pre><code class="language-ruby">def destroy_pipeline_artifacts_not_checksummable
  artifacts = Ci::PipelineArtifact.verification_failed.where(&quot;verification_failure like '%File is not checksummable%'&quot;)
  puts &quot;Found #{artifacts.count} pipeline artifacts that failed verification with 'File is not checksummable'.&quot;
  puts &quot;Enter 'y' to continue: &quot;
  prompt = STDIN.gets.chomp
  if prompt != 'y'
    puts &quot;Exiting without action...&quot;
    return
  end

  puts &quot;Destroying all...&quot;
  artifacts.destroy_all
  puts &quot;Done!&quot;
end

destroy_pipeline_artifacts_not_checksummable
</code></pre>
<h3>Error: <code>Projects - Error during verification: Repository does not exist</code></h3>
<p><strong>Root Cause:</strong> Projects without Git repositories causing verification failures.</p>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Projects display "Repository does not exist" errors during verification</li>
<li>False error reporting in Geo UI for projects that legitimately have no repositories</li>
<li>Wasted sync attempts on non-existent repositories</li>
</ul>
<p><strong>Workaround:</strong></p>
<p>Create project repositories on the primary when they don't exist:</p>
<pre><code class="language-ruby">puts &quot;Found #{Project.verification_failed.count} project repos failed to checksum&quot;
Project.verification_failed.find_each do |p|
  puts &quot;#{p.full_path} #{p.ensure_repository.inspect}&quot;
end
</code></pre>
<h3>Error: <code>Expected(200) &lt;=&gt; Actual(403 Forbidden)</code></h3>
<p><strong>Root Cause:</strong> Missing <code>ListBucket</code> permission causing S3 API to return 403 instead of 404.</p>
<p><strong>Symptoms:</strong></p>
<ul>
<li>403 errors in logs with S3 endpoints</li>
<li>HEAD requests failing to S3 buckets</li>
<li>Sync failures for object storage-backed data types</li>
</ul>
<p><strong>Resolution:</strong></p>
<p>This requires infrastructure team intervention to add the <code>ListBucket</code> permission to the S3 IAM policy used by GitLab.</p>
<h3>Message: <code>Synchronization failed - Error syncing repository</code></h3>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>If large repositories are affected by this problem,
their resync may take a long time and cause significant load on your Geo sites,
storage and network systems.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>The following error message indicates a consistency check error when syncing the repository:</p>
<pre><code class="language-plaintext">Synchronization failed - Error syncing repository [..] fatal: fsck error in packed object
</code></pre>
<p>Several issues can trigger this error. For example, problems with email addresses:</p>
<pre><code class="language-plaintext">Error syncing repository: 13:fetch remote: &quot;error: object &lt;SHA&gt;: badEmail: invalid author/committer line - bad email
   fatal: fsck error in packed object
   fatal: fetch-pack: invalid index-pack output
</code></pre>
<p>Another issue that can trigger this error is <code>object &lt;SHA&gt;: hasDotgit: contains '.git'</code>. Check the specific errors because you might have more than one problem across all
your repositories.</p>
<p>A second synchronization error can also be caused by repository check issues:</p>
<pre><code class="language-plaintext">Error syncing repository: 13:Received RST_STREAM with error code 2.
</code></pre>
<p>These errors can be observed by <a href="#sync-all-resources-of-one-component-that-failed-to-sync">immediately syncing all failed repositories</a>.</p>
<p>Removing the malformed objects causing consistency errors involves rewriting the repository history, which is usually not an option.</p>
<p>To ignore these consistency checks, reconfigure Gitaly <strong>on the secondary Geo sites</strong> to ignore these <code>git fsck</code> issues.
The following configuration example:</p>
<ul>
<li><a href="../../../../update/versions/gitlab_16_changes.md#gitaly-configuration-structure-change">Uses the new configuration structure</a>
  required from GitLab 16.0.</li>
<li>Ignores five common check failures.</li>
</ul>
<p><a href="../../../gitaly/consistency_checks.html">The Gitaly documentation has more details</a>
about other Git check failures and earlier versions of GitLab.</p>
<pre><code class="language-ruby">gitaly['configuration'] = {
  git: {
    config: [
      { key: &quot;fsck.duplicateEntries&quot;, value: &quot;ignore&quot; },
      { key: &quot;fsck.badFilemode&quot;, value: &quot;ignore&quot; },
      { key: &quot;fsck.missingEmail&quot;, value: &quot;ignore&quot; },
      { key: &quot;fsck.badEmail&quot;, value: &quot;ignore&quot; },
      { key: &quot;fsck.hasDotgit&quot;, value: &quot;ignore&quot; },
      { key: &quot;fetch.fsck.duplicateEntries&quot;, value: &quot;ignore&quot; },
      { key: &quot;fetch.fsck.badFilemode&quot;, value: &quot;ignore&quot; },
      { key: &quot;fetch.fsck.missingEmail&quot;, value: &quot;ignore&quot; },
      { key: &quot;fetch.fsck.badEmail&quot;, value: &quot;ignore&quot; },
      { key: &quot;fetch.fsck.hasDotgit&quot;, value: &quot;ignore&quot; },
      { key: &quot;receive.fsck.duplicateEntries&quot;, value: &quot;ignore&quot; },
      { key: &quot;receive.fsck.badFilemode&quot;, value: &quot;ignore&quot; },
      { key: &quot;receive.fsck.missingEmail&quot;, value: &quot;ignore&quot; },
      { key: &quot;receive.fsck.badEmail&quot;, value: &quot;ignore&quot; },
      { key: &quot;receive.fsck.hasDotgit&quot;, value: &quot;ignore&quot; },
    ],
  },
}
</code></pre>
<p>A comprehensive list of <code>fsck</code> errors can be found in the <a href="https://git-scm.com/docs/git-fsck#_fsck_messages">Git documentation</a>.</p>
<p>GitLab 16.1 and later <a href="https://gitlab.com/gitlab-org/gitaly/-/merge_requests/5879">include an enhancement</a> that might resolve some of these issues.</p>
<p><a href="https://gitlab.com/gitlab-org/gitaly/-/issues/5625">Gitaly issue 5625</a> proposes to ensure that Geo replicates repositories even if the source repository contains
problematic commits.</p>
<h3>Related error <code>does not appear to be a git repository</code></h3>
<p>You can also get the error message <code>Synchronization failed - Error syncing repository</code> along with the following log messages.
This error indicates that the expected Geo remote is not present in the <code>.git/config</code> file
of a repository on the secondary Geo site's file system:</p>
<pre><code class="language-json">{
  &quot;created&quot;: &quot;@1603481145.084348757&quot;,
  &quot;description&quot;: &quot;Error received from peer unix:/var/opt/gitlab/gitaly/gitaly.socket&quot;,
  
  &quot;grpc_message&quot;: &quot;exit status 128&quot;,
  &quot;grpc_status&quot;: 13
}
{  
  &quot;grpc.request.fullMethod&quot;: &quot;/gitaly.RemoteService/FindRemoteRootRef&quot;,
  &quot;grpc.request.glProjectPath&quot;: &quot;&lt;namespace&gt;/&lt;project&gt;&quot;,
  
  &quot;level&quot;: &quot;error&quot;,
  &quot;msg&quot;: &quot;fatal: 'geo' does not appear to be a git repository
          fatal: Could not read from remote repository. &quot;,
}
</code></pre>
<p>To solve this:</p>
<ol>
<li>
<p>Sign in on the web interface for the secondary Geo site.</p>
</li>
<li>
<p>Back up <a href="../../../repository_storage_paths.md#translate-hashed-storage-paths">the <code>.git</code> folder</a>.</p>
</li>
<li>
<p>Optional. <a href="../../../logs/log_parsing.md#find-all-projects-affected-by-a-fatal-git-problem">Spot-check</a>
   a few of those IDs whether they indeed correspond
   to a project with known Geo replication failures.
   Use <code>fatal: 'geo'</code> as the <code>grep</code> term and the following API call:</p>
</li>
</ol>
<p><code>shell
   curl --request GET --header "PRIVATE-TOKEN: &lt;your_access_token&gt;" "https://gitlab.example.com/api/v4/projects/&lt;first_failed_geo_sync_ID&gt;"</code></p>
<ol>
<li>Enter the <a href="../../../operations/rails_console.html">Rails console</a> and run:</li>
</ol>
<p>```ruby
   failed_project_registries = Geo::ProjectRepositoryRegistry.failed</p>
<p>if failed_project_registries.any?
     puts "Found #{failed_project_registries.count} failed project repository registry entries:"</p>
<pre><code> failed_project_registries.each do |registry|
   puts "ID: #{registry.id}, Project ID: #{registry.project_id}, Last Sync Failure: '#{registry.last_sync_failure}'"
 end
</code></pre>
<p>else
     puts "No failed project repository registry entries found."
   end
   ```</p>
<ol>
<li>Run the following commands to execute a new sync for each project:</li>
</ol>
<p><code>ruby
   failed_project_registries.each do |registry|
     registry.replicator.sync
     puts "Sync initiated for registry ID: #{registry.id}, Project ID: #{registry.project_id}"
   end</code></p>
<h2>Failures during backfill</h2>
<p>During a <a href="../../_index.md#backfill">backfill</a>, failures are scheduled to be retried at the end
of the backfill queue, therefore these failures only clear up <strong>after</strong> the backfill completes.</p>
<h2>Message: <code>unexpected disconnect while reading sideband packet</code></h2>
<p>Unstable networking conditions can cause Gitaly to fail when trying to fetch large repository
data from the primary site. Those conditions can result in this error:</p>
<pre><code class="language-plaintext">curl 18 transfer closed with outstanding read data remaining &amp; fetch-pack:
unexpected disconnect while reading sideband packet
</code></pre>
<p>This error is more likely to happen if a repository has to be
replicated from scratch between sites.</p>
<p>Geo retries several times, but if the transmission is consistently interrupted
by network hiccups, an alternative method such as <code>rsync</code> can be used to circumvent <code>git</code> and
create the initial copy of any repository that fails to be replicated by Geo.</p>
<p>We recommend transferring each failing repository individually and checking for consistency
after each transfer. Follow the <a href="../../../operations/moving_repositories.md#use-rsync-to-another-server"><code>rsync</code> to another server instructions</a>
to transfer each affected repository from the primary to the secondary site.</p>
<h2>Find repository check failures in a Geo secondary site</h2>
<p>{{&lt; alert type="note" &gt;}}</p>
<p>All repositories data types have been migrated to the Geo Self-Service Framework in GitLab 16.3. There is an <a href="https://gitlab.com/gitlab-org/gitlab/-/issues/426659">issue to implement this functionality back in the Geo Self-Service Framework</a>.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>For GitLab 16.2 and earlier:</p>
<p>When <a href="../../../repository_checks.md#enable-repository-checks-for-all-projects">enabled for all projects</a>, <a href="../../../repository_checks.html">Repository checks</a> are also performed on Geo secondary sites. The metadata is stored in the Geo tracking database.</p>
<p>Repository check failures on a Geo secondary site do not necessarily imply a replication problem. Here is a general approach to resolve these failures.</p>
<ol>
<li>Find affected repositories as mentioned below, as well as their <a href="../../../repository_checks.md#what-to-do-if-a-check-failed">logged errors</a>.</li>
<li>Try to diagnose specific <code>git fsck</code> errors. The range of possible errors is wide, try putting them into search engines.</li>
<li>Test typical functions of the affected repositories. Pull from the secondary, view the files.</li>
<li>Check if the primary site's copy of the repository has an identical <code>git fsck</code> error. If you are planning a failover, then consider prioritizing that the secondary site has the same information that the primary site has. Ensure you have a backup of the primary, and follow <a href="../../disaster_recovery/planned_failover.html">planned failover guidelines</a>.</li>
<li>Push to the primary and check if the change gets replicated to the secondary site.</li>
<li>If replication is not automatically working, try to manually sync the repository.</li>
</ol>
<p><a href="../../../operations/rails_console.md#starting-a-rails-console-session">Start a Rails console session</a>
to enact the following, basic troubleshooting steps.</p>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>Commands that change data can cause damage if not run correctly or under the right conditions. Always run commands in a test environment first and have a backup instance ready to restore.</p>
<p>{{&lt; /alert &gt;}}</p>
<h3>Get the number of repositories that failed the repository check</h3>
<pre><code class="language-ruby">Geo::ProjectRegistry.where(last_repository_check_failed: true).count
</code></pre>
<h3>Find the repositories that failed the repository check</h3>
<pre><code class="language-ruby">Geo::ProjectRegistry.where(last_repository_check_failed: true)
</code></pre>
<h2>Hard delete a repository from Gitaly Cluster and resync</h2>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>This procedure is risky, and heavy-handed. Use it as a last resort only when other
troubleshooting methods have failed. This procedure causes temporary data loss until the
repository is resynced.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>This procedure deletes the repository from the secondary site's Gitaly cluster, and re-syncs it.
You should consider using it only if you understand the risks, and if these conditions are all true:</p>
<ul>
<li><code>git clone</code> is working for a repository on the primary site.</li>
<li><code>p.replicator.sync_repository</code> (where <code>p</code> is a project model instance) logs a Gitaly error on a secondary site.</li>
<li>Standard troubleshooting has not resolved the issue.</li>
</ul>
<p>Prerequisites:</p>
<ul>
<li>Ensure you have administrative access to both the secondary site's Rails console and Praefect nodes.</li>
<li>Verify that the repository is accessible and working correctly on the primary site.</li>
<li>Have a backup plan in case you must reverse this procedure.</li>
</ul>
<p>To do this:</p>
<ol>
<li>Sign in to the Rails console in the secondary site.</li>
<li>
<p>Instantiate a project model, and save it to a variable <code>p</code>, using one of these options:</p>
</li>
<li>
<p>If you know the affected project ID (for example, <code>60087</code>):</p>
<p><code>ruby
 p = Project.find(60087)</code></p>
</li>
<li>
<p>If you know the affected project path in GitLab (for example, <code>my-group/my-project</code>):</p>
<p><code>ruby
 p = Project.find_by_full_path('my-group/my-project')</code></p>
</li>
<li>
<p>Output the project Git repository's virtual storage, and note it for later:</p>
</li>
</ol>
<p><code>ruby
   p.repository.storage</code></p>
<p>Example output:</p>
<p><code>ruby
   irb(main):002:0&gt; p.repository.storage
   =&gt; "default"</code></p>
<ol>
<li>Output the project Git repository's relative path, and note it for later:</li>
</ol>
<p><code>ruby
   p.repository.disk_path + '.git'</code></p>
<p>Example output:</p>
<p><code>ruby
   irb(main):003:0&gt; p.repository.disk_path + '.git'
   =&gt; "@hashed/66/b2/66b2fc8562b3432399acc2d0108fcd2782b32bd31d59226c7a03a20b32c76ee8.git"</code></p>
<ol>
<li>SSH into a Praefect node in the secondary site.</li>
<li>Follow the procedure to
   <a href="../../../gitaly/praefect/recovery.md#manually-remove-repositories">Manually remove repositories from Gitaly Cluster</a>,
   using the virtual storage and relative path you noted in the previous steps.</li>
</ol>
<p>The Git repository on the secondary site is now deleted.</p>
<ol>
<li>In the Rails console, before you resync, set a correlation ID. This ID helps you search all logs
   related to the commands you run in this session:</li>
</ol>
<p><code>ruby
   Gitlab::ApplicationContext.push({})</code></p>
<p>Example output:</p>
<p><code>ruby
   [2] pry(main)&gt; Gitlab::ApplicationContext.push({})
   =&gt; #&lt;Labkit::Context:0x0000000122aa4060 @data={"correlation_id"=&gt;"53da64ae800bd4794a2b61ab1c80b028"}&gt;</code></p>
<ol>
<li>Sync the project Git repository:</li>
</ol>
<p><code>ruby
   p.replicator.sync_repository</code></p>
<p>The Git repository should now be resynced from the primary site to the secondary site. Monitor the sync
process through the Geo admin interface, or by checking the repository's sync status in the Rails console.</p>
<h2>Stuck sync or verification counts due to worker errors</h2>
<p>If <code>BulkMarkPendingBatchWorker</code> or <code>BulkMarkVerificationPendingBatchWorker</code> is throwing errors like "Cannot obtain exclusive lease" and repeatedly throwing errors, and sync/verification counts are stuck, you can simply drop the Sidekiq jobs, which will recover. This happens after "resync all" or "reverify all" operations at times. For more information, see <a href="https://gitlab.com/gitlab-org/gitlab/-/issues/573613">issue 573613</a> for details.</p>
<p>To drop the jobs for synchronization issues:</p>
<pre><code class="language-ruby">Feature.enable(:&quot;drop_sidekiq_jobs_Geo::BulkMarkPendingBatchWorker&quot;)
</code></pre>
<p>For verification issues, use:</p>
<pre><code class="language-ruby">Feature.enable(:&quot;drop_sidekiq_jobs_Geo::BulkMarkVerificationPendingBatchWorker&quot;)
</code></pre>
<h2>Infrastructure and performance considerations</h2>
<p>Some synchronization issues are caused by infrastructure-level problems or performance constraints.</p>
<h3>High concurrency issues</h3>
<p>Excessive Geo verification concurrency can overwhelm the database and cause sync failures.</p>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Database connection timeouts</li>
<li>High CPU usage on database servers</li>
<li>Slow sync progress despite healthy infrastructure</li>
</ul>
<p><strong>Diagnosis and Resolution:</strong></p>
<p>Reduce concurrency settings on the <strong>primary</strong> site via <a href="../tuning.md#changing-the-syncverification-concurrency-values">UI</a></p>
<h2>Manual sync status updates</h2>
<p>In some cases, you may need to manually mark an object type as synced after resolving underlying issues. This scenario occurs when the issue can only be fixed via a manual upload of the file to the object bucket in the secondary site. Normally that operation should not be needed, but can happen due to version bugs. The following shows a way to mark those manually uploaded object types (in this case uploads) as synced.</p>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>Only mark objects as synced if you have verified that the files are actually present and accessible on the secondary site.</p>
<p>{{&lt; /alert &gt;}}</p>
<pre><code class="language-ruby">def mark_upload_synced(upload_id)
  upload = Upload.find(upload_id)
  registry = upload.replicator.registry
  registry.start
  registry.synced!
  puts &quot;Marked upload #{upload_id} as synced&quot;
end

# Mark specific uploads as synced
upload_ids = [107221, 107320] # Replace with actual IDs
upload_ids.each { |id| mark_upload_synced(id) }
</code></pre>
<h2>Resetting Geo <strong>secondary</strong> site replication</h2>
<p>If you get a <strong>secondary</strong> site in a broken state and want to reset the replication state
to start again from scratch, there are a few steps that can help you:</p>
<ol>
<li>Stop Sidekiq and the Geo Log Cursor.</li>
</ol>
<p>It's possible to make Sidekiq stop gracefully, but making it stop getting new jobs and
   wait until the current jobs to finish processing.</p>
<p>You need to send a <strong>SIGTSTP</strong> kill signal for the first phase and them a <strong>SIGTERM</strong>
   when all jobs have finished. Otherwise just use the <code>gitlab-ctl stop</code> commands.</p>
<p>```shell
   gitlab-ctl status sidekiq
   # run: sidekiq: (pid 10180) &lt;- this is the PID you will use
   kill -TSTP 10180 # change to the correct PID</p>
<p>gitlab-ctl stop sidekiq
   gitlab-ctl stop geo-logcursor
   ```</p>
<p>You can watch the <a href="../../../logs/_index.md#sidekiq-logs">Sidekiq logs</a> to know when Sidekiq jobs processing has finished:</p>
<p><code>shell
   gitlab-ctl tail sidekiq</code></p>
<ol>
<li>Clear Gitaly and Gitaly Cluster (Praefect) data.</li>
</ol>
<p>{{&lt; tabs &gt;}}</p>
<p>{{&lt; tab title="Gitaly" &gt;}}</p>
<p><code>shell
   mv /var/opt/gitlab/git-data/repositories /var/opt/gitlab/git-data/repositories.old
   sudo gitlab-ctl reconfigure</code></p>
<p>{{&lt; /tab &gt;}}</p>
<p>{{&lt; tab title="Gitaly Cluster (Praefect)" &gt;}}</p>
<ol>
<li>Optional. Disable the Praefect internal load balancer.</li>
<li>
<p>Stop Praefect on each Praefect server:</p>
<p><code>shell
  sudo gitlab-ctl stop praefect</code></p>
</li>
<li>
<p>Reset the Praefect database:</p>
<p><code>shell
  sudo /opt/gitlab/embedded/bin/psql -U praefect -d template1 -h localhost -c "DROP DATABASE praefect_production WITH (FORCE);"
  sudo /opt/gitlab/embedded/bin/psql -U praefect -d template1 -h localhost -c "CREATE DATABASE praefect_production WITH OWNER=praefect ENCODING=UTF8;"</code></p>
</li>
<li>
<p>Rename/delete repository data from each Gitaly node:</p>
<p><code>shell
  sudo mv /var/opt/gitlab/git-data/repositories /var/opt/gitlab/git-data/repositories.old
  sudo gitlab-ctl reconfigure</code></p>
</li>
<li>
<p>On your Praefect deploy node run reconfigure to set up the database:</p>
<p><code>shell
  sudo gitlab-ctl reconfigure</code></p>
</li>
<li>
<p>Start Praefect on each Praefect server:</p>
<p><code>shell
  sudo gitlab-ctl start praefect</code></p>
</li>
<li>
<p>Optional. If you disabled it, reactivate the Praefect internal load balancer.</p>
</li>
</ol>
<p>{{&lt; /tab &gt;}}</p>
<p>{{&lt; /tabs &gt;}}</p>
<p>{{&lt; alert type="note" &gt;}}</p>
<p>You may want to remove the <code>/var/opt/gitlab/git-data/repositories.old</code> in the future
   as soon as you confirmed that you don't need it anymore, to save disk space.</p>
<p>{{&lt; /alert &gt;}}</p>
<ol>
<li>Optional. Rename other data folders and create new ones.</li>
</ol>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>You may still have files on the <strong>secondary</strong> site that have been removed from the <strong>primary</strong> site, but this
   removal has not been reflected. If you skip this step, these files are not removed from the Geo <strong>secondary</strong> site.</p>
<p>{{&lt; /alert &gt;}}</p>
<p>Any uploaded content (like file attachments, avatars, or LFS objects) is stored in a
   subfolder in one of these paths:</p>
<ul>
<li><code>/var/opt/gitlab/gitlab-rails/shared</code></li>
<li><code>/var/opt/gitlab/gitlab-rails/uploads</code></li>
</ul>
<p>To rename all of them:</p>
<p>```shell
   gitlab-ctl stop</p>
<p>mv /var/opt/gitlab/gitlab-rails/shared /var/opt/gitlab/gitlab-rails/shared.old
   mkdir -p /var/opt/gitlab/gitlab-rails/shared</p>
<p>mv /var/opt/gitlab/gitlab-rails/uploads /var/opt/gitlab/gitlab-rails/uploads.old
   mkdir -p /var/opt/gitlab/gitlab-rails/uploads</p>
<p>gitlab-ctl start postgresql
   gitlab-ctl start geo-postgresql
   ```</p>
<p>Reconfigure to recreate the folders and make sure permissions and ownership
   are correct:</p>
<p><code>shell
   gitlab-ctl reconfigure</code></p>
<ol>
<li>Reset the Tracking Database.</li>
</ol>
<p>{{&lt; alert type="warning" &gt;}}</p>
<p>If you skipped the optional step 3, be sure both <code>geo-postgresql</code> and <code>postgresql</code> services are running.</p>
<p>{{&lt; /alert &gt;}}</p>
<p><code>shell
   gitlab-rake db:drop:geo DISABLE_DATABASE_ENVIRONMENT_CHECK=1   # on a secondary app node
   gitlab-ctl reconfigure     # on the tracking database node
   gitlab-rake db:migrate:geo # on a secondary app node</code></p>
<ol>
<li>Restart previously stopped services.</li>
</ol>
<p><code>shell
   gitlab-ctl start</code></p></code></pre>
</body>
</html>
